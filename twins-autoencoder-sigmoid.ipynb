{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nfrom copy import deepcopy\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\nfrom itertools import repeat\nimport collections.abc","metadata":{"_uuid":"d46a233a-d71a-480c-9614-cea1875dd3dd","_cell_guid":"1130a6ce-3175-461a-9c1f-90b79862f8f7","collapsed":false,"id":"OUn97ZAnh6st","execution":{"iopub.status.busy":"2022-08-08T19:36:02.109712Z","iopub.execute_input":"2022-08-08T19:36:02.110317Z","iopub.status.idle":"2022-08-08T19:36:04.130622Z","shell.execute_reply.started":"2022-08-08T19:36:02.110222Z","shell.execute_reply":"2022-08-08T19:36:04.129485Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prequisite classes","metadata":{"_uuid":"0d885729-b83f-45fd-997f-a1d8cfe75ab8","_cell_guid":"b170ebeb-adce-4cea-a75f-023d531c1e08","id":"Fnnqzjp5h6sx","trusted":true}},{"cell_type":"code","source":"class Mlp(nn.Module):\n    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n    \"\"\"\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, bias=True, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = to_2tuple(bias)\n        drop_probs = to_2tuple(drop)\n\n        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias[0])\n        self.act = act_layer()\n        self.drop1 = nn.Dropout(drop_probs[0])\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1])\n        self.drop2 = nn.Dropout(drop_probs[1])\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop1(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n            return x\n        return tuple(repeat(x, n))\n    return parse\n\nto_2tuple = _ntuple(2)\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.scale_by_keep = scale_by_keep\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n\n    def extra_repr(self):\n        return f'drop_prob={round(self.drop_prob,3):0.3f}'","metadata":{"_uuid":"926b9bc2-7778-405b-976b-ee05476653a1","_cell_guid":"1109fb8d-2a22-494f-8543-f0d1f8129f02","collapsed":false,"id":"drqPgEtMh6sy","execution":{"iopub.status.busy":"2022-08-08T19:36:04.132922Z","iopub.execute_input":"2022-08-08T19:36:04.133779Z","iopub.status.idle":"2022-08-08T19:36:04.149498Z","shell.execute_reply.started":"2022-08-08T19:36:04.133739Z","shell.execute_reply":"2022-08-08T19:36:04.148524Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # **Paper: [Twins Transformer](http://arxiv.org/abs/2104.13840)**","metadata":{"_uuid":"a29635d2-824b-4b16-a786-0abf3d95191a","_cell_guid":"91473488-bc47-4a69-871c-66bde8f4c3ff","trusted":true}},{"cell_type":"code","source":"Size_ = Tuple[int, int]\n\nclass LocallyGroupedAttn(nn.Module):\n    \"\"\" LSA: self attention within a group\n    \"\"\"\n    def __init__(self, dim, num_heads=8, attn_drop=0., proj_drop=0., ws=1):\n        assert ws != 1\n        super(LocallyGroupedAttn, self).__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.ws = ws\n\n    def forward(self, x, size: Size_):\n        # There are two implementations for this function, zero padding or mask. We don't observe obvious difference for\n        # both. You can choose any one, we recommend forward_padding because it's neat. However,\n        # the masking implementation is more reasonable and accurate.\n        B, N, C = x.shape\n        # x: B,H*W,C\n        H, W = size\n        x = x.view(B, H, W, C) # x: B, H ,W , C\n        pad_l = pad_t = 0\n        pad_r = (self.ws - W % self.ws) % self.ws\n        pad_b = (self.ws - H % self.ws) % self.ws\n        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b)) # padds last 3 dimensions, channel by 0,0, height by pad_l, pad_r, width by pad_t, pad_b\n        # Hp = H + pad_r, Wp = W + pad_b \n        _, Hp, Wp, _ = x.shape\n        _h, _w = Hp // self.ws, Wp // self.ws #_h and _w will be the number of the padded smaller windows horizontally and vertically on 2D\n        x = x.reshape(B, _h, self.ws, _w, self.ws, C).transpose(2, 3) #B,_h,_w,patches in vertical windows, patches in horizontal windows, channels for each patch\n        #qkv: each of q k v, B, _h*_w, heads, num_windows,  dim_head\n        qkv = self.qkv(x).reshape(\n            B, _h * _w, self.ws * self.ws, 3, self.num_heads, C // self.num_heads).permute(3, 0, 1, 4, 2, 5)\n        q, k, v = qkv[0], qkv[1], qkv[2] # q,k,v: B, _h*_w, heads, num_patches, dim_head\n        attn = (q @ k.transpose(-2, -1)) * self.scale # attn: B, _h * _w, heads, num_patches,num_patches -- dot product between each patch in each window\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        attn = (attn @ v).transpose(2, 3).reshape(B, _h, _w, self.ws, self.ws, C) # Attention is done, we get attn: B, _h, _w,  num_patches , num_patches, channels\n        x = attn.transpose(2, 3).reshape(B, _h * self.ws, _w * self.ws, C) # getting back the whole image again with x: B, padded height, padded width, channels\n        if pad_r > 0 or pad_b > 0: # If padded somehow, remove the elements in the places of paddings\n            x = x[:, :H, :W, :].contiguous() # x: B, H, W, C\n        x = x.reshape(B, N, C) # x: B, H*W, C\n        x = self.proj(x) #projection of the x tensor to dim channels I DID NOT NDERSTAND WHY EXACTLY\n        x = self.proj_drop(x)\n        return x\n    \nclass GlobalSubSampleAttn(nn.Module):\n    \"\"\" GSA: using a  key to summarize the information for a group to be efficient.\n    \"\"\"\n\n    def __init__(self, dim, num_heads=8, attn_drop=0., proj_drop=0., sr_ratio=1):\n        super().__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        self.q = nn.Linear(dim, dim, bias=True)\n        self.kv = nn.Linear(dim, dim * 2, bias=True)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.sr_ratio = sr_ratio\n        if sr_ratio > 1:\n            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n            self.norm = nn.LayerNorm(dim)\n        else:\n            self.sr = None\n            self.norm = None\n\n    def forward(self, x, size: Size_):\n        B, N, C = x.shape\n        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3) # q: B,num_heads, H*W, head_dim\n        # We get the queries without supsampling x\n        if self.sr is not None:\n            x = x.permute(0, 2, 1).reshape(B, C, *size) # B, C, H, W\n            x = self.sr(x).reshape(B, C, -1).permute(0, 2, 1) # x: B, C, H/sr, W/sr -> B, C, H/sr*W/sr -- Subsampling\n            x = self.norm(x)\n        # After subsampling we get the keys and the values\n        kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) # k and v, B, num_heads, H/sr * W/sr, head_dim\n        k, v = kv[0], kv[1] # k and v : B, num_heads, H/sr * W/sr, head_dim\n        # q: B,num_heads, H*W, head_dim\n        # k.transpose(-2,-1) : B, num_heads, head_dim, H/sr * W/sr\n        attn = (q @ k.transpose(-2, -1)) * self.scale # attn: B, num_heads, H*W, H/sr * W/sr\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        # attn: B, num_heads, H*W, H/sr * W/sr\n        # v : B, num_heads, H/sr * W/sr, head_dim\n        # (attn @ v) : B, num_heads, H*W, head_dim\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C) # x: B, H*W, C == head_dim * num_heads\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(\n            self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0., drop_path=0.,\n            act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1, ws=None):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        if ws is None:\n            self.attn = Attention(dim, num_heads, False, None, attn_drop, drop)\n        elif ws == 1:\n            self.attn = GlobalSubSampleAttn(dim, num_heads, attn_drop, drop, sr_ratio)\n        else:\n            self.attn = LocallyGroupedAttn(dim, num_heads, attn_drop, drop, ws)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x, size: Size_):\n        x = x + self.drop_path(self.attn(self.norm1(x), size))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x # x: B, H*W, C\n    \nclass PosConv(nn.Module):\n    # PEG  from https://arxiv.org/abs/2102.10882\n    def __init__(self, in_chans, embed_dim=768, stride=1):\n      # in_chans: Image channels after patch embedding, embed_dim: channels after projection, in_channels must be divisible by embed_dim\n        super(PosConv, self).__init__()\n        self.proj = nn.Sequential(nn.Conv2d(in_chans, embed_dim, 3, stride, 1, bias=True, groups=embed_dim), )\n        self.stride = stride\n\n    def forward(self, x, size: Size_):\n        B, N, C = x.shape\n        cnn_feat_token = x.transpose(1, 2).view(B, C, *size) # cnn_feat_token: B, C, H,W\n        x = self.proj(cnn_feat_token) # x: B, embed_dim, H,W -- due to stride and size 3 kernel\n        if self.stride == 1: # Apparently for this to work class PatchEmbed should output an embedding with the same dimensions as PosCov\n            x += cnn_feat_token\n        x = x.flatten(2).transpose(1, 2) # x: B, H*W, embed_dim\n        return x\n    \nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size) # img_size = (img_size, img_size)\n        patch_size = to_2tuple(patch_size) # patch_size = (patch_size, patch_size)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n            f\"img_size {img_size} should be divided by patch_size {patch_size}.\"\n        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1] # H,W: number of patches in vertical and horizontal directions respectively as a 2D map -- Considering each patch as a pixel, H and W makes sense\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) # Nonoverlapping convolutions to embed each patch\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x) -> Tuple[torch.Tensor, Size_]:\n        B, C, H, W = x.shape # Apparently first input is an image with dimensions: B, C, orig_H, orig_W\n\n        x = self.proj(x).flatten(2).transpose(1, 2) # x: B, H'*W', embed_dim -- H' and W' are the new image where every pixel is actually a patch with 768 dimensions\n        x = self.norm(x)\n        out_size = (H // self.patch_size[0], W // self.patch_size[1])\n\n        return x, out_size # Return both the patch embedded x and its new size\n    \nclass Twins(nn.Module):\n    \"\"\" Twins Vision Transfomer (Revisiting Spatial Attention)\n    Adapted from PVT (PyramidVisionTransformer) class at https://github.com/whai362/PVT.git\n    \"\"\"\n    def __init__(\n            self, img_size=32, patch_size=4, in_chans=3, num_classes=1000, global_pool='avg',\n            embed_dims=(64, 128, 256, 512), num_heads=(1, 2, 4, 8), mlp_ratios=(4, 4, 4, 4), depths=(3, 4, 6, 3),\n            sr_ratios=(8, 4, 2, 1), wss=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n            norm_layer=partial(nn.LayerNorm, eps=1e-6), block_cls=Block):\n        super().__init__()\n        self.num_classes = num_classes # For image classification\n        self.global_pool = global_pool\n        self.depths = depths\n        self.embed_dims = embed_dims\n        self.num_features = embed_dims[-1]\n        self.grad_checkpointing = False\n\n        img_size = to_2tuple(img_size)\n        prev_chs = in_chans # original image channels\n        self.patch_embeds = nn.ModuleList()\n        self.pos_drops = nn.ModuleList() # For dropout\n        for i in range(len(depths)):\n            self.patch_embeds.append(PatchEmbed(img_size, patch_size, prev_chs, embed_dims[i])) # Initilialize the PatchEmbed modules with different embed_dim for each layer\n            self.pos_drops.append(nn.Dropout(p=drop_rate))\n            prev_chs = embed_dims[i] # Update the prev_chs so we can use PatchEmbed modules\n            img_size = tuple(t // patch_size for t in img_size) # Gradual patching, declaring the new img sizes while passing through each layer\n            patch_size = 2\n\n        self.blocks = nn.ModuleList()\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n        cur = 0\n        for k in range(len(depths)):\n            _block = nn.ModuleList([block_cls(\n                dim=embed_dims[k], num_heads=num_heads[k], mlp_ratio=mlp_ratios[k], drop=drop_rate,\n                attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer, sr_ratio=sr_ratios[k],\n                ws=1 if wss is None or i % 2 == 1 else wss[k]) for i in range(depths[k])]) # In a layer with depth 3 for example the i=0 th layer will do LSA and i = 1 will do GloalSubsampled\n            self.blocks.append(_block)\n            cur += depths[k]\n\n        self.pos_block = nn.ModuleList([PosConv(embed_dim, embed_dim) for embed_dim in embed_dims])\n\n        self.norm = norm_layer(self.num_features)\n\n    def forward(self, x):\n        B = x.shape[0]\n        for i, (embed, drop, blocks, pos_blk) in enumerate(\n                zip(self.patch_embeds, self.pos_drops, self.blocks, self.pos_block)):\n            x, size = embed(x) # First create and embed the patches\n            x = drop(x)\n            for j, blk in enumerate(blocks): # Pass thorugh attention modules but after the first layer add PEG \n                x = blk(x, size) \n                if j == 0:\n                    x = pos_blk(x, size)  # PEG here\n            if i < len(self.depths) - 1: # Reshape the x to B, C, H, W unless it is the final output tensor, the final x is x: B, last_H*last_W, last_embedding_dim\n                x = x.reshape(B, *size, -1).permute(0, 3, 1, 2).contiguous()\n        x = self.norm(x)\n        x = x.reshape(B, *size, -1).permute(0, 3, 1, 2).contiguous()\n        return x","metadata":{"_uuid":"9ba31464-2b86-4f4d-a987-fcd94955dd72","_cell_guid":"d1b87cb5-a8f9-449a-b93a-78a397a225aa","collapsed":false,"id":"NgTgRY0Hh6sz","execution":{"iopub.status.busy":"2022-08-08T19:36:04.151428Z","iopub.execute_input":"2022-08-08T19:36:04.152241Z","iopub.status.idle":"2022-08-08T19:36:04.218261Z","shell.execute_reply.started":"2022-08-08T19:36:04.152207Z","shell.execute_reply":"2022-08-08T19:36:04.217171Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"proj = nn.ConvTranspose2d(512,512,3,1,1,bias=True)\ntest = torch.rand(5,512,32,32)\noup = proj(test)\nprint(oup.shape)","metadata":{"_uuid":"20f776b8-82d9-4a73-93ed-551cd188afda","_cell_guid":"f3a3c31a-ca5f-4248-ab07-bcfbafad9930","collapsed":false,"id":"uOP863HSs4b1","outputId":"f17647be-4593-47ec-aee9-3ce888a44762","execution":{"iopub.status.busy":"2022-08-08T19:36:04.221662Z","iopub.execute_input":"2022-08-08T19:36:04.222462Z","iopub.status.idle":"2022-08-08T19:36:04.736334Z","shell.execute_reply.started":"2022-08-08T19:36:04.222426Z","shell.execute_reply":"2022-08-08T19:36:04.735291Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Inv_PosConv(nn.Module):\n  def __init__(self,in_chans,embed_dim,stride=1):\n    super(Inv_PosConv,self).__init__()\n    self.proj = nn.Sequential(nn.ConvTranspose2d(in_chans, embed_dim, 3, stride, 1, bias=True, groups=embed_dim),)\n    self.stride = stride\n\n  def forward(self,x,size:Size_):\n    B,N,C = x.shape\n    # print(x.shape)\n    cnn_feat_token = x.transpose(1, 2).view(B, C, *size) # cnn_feat_token: B, C, H,W\n    # print(cnn_feat_token.shape)\n    x = self.proj(cnn_feat_token) # x: B, embed_dim, H,W -- due to stride and size 3 kernel\n    # print(x.shape)\n    if self.stride == 1: # Apparently for this to work class PatchEmbed should output an embedding with the same dimensions as PosCov\n        x += cnn_feat_token\n    x = x.flatten(2).transpose(1, 2) # x: B, H*W, embed_dim\n    return x\n\nclass Inv_PatchEmbed(nn.Module):\n  def __init__(self,img_size = 32, patch_size = 2, in_chans = 3 ,embed_dim = 768):\n    super().__init__()\n    img_size = to_2tuple(img_size) # img_size = (img_size, img_size)\n    patch_size = to_2tuple(patch_size) # patch_size = (patch_size, patch_size)\n\n    self.img_size = img_size\n    self.patch_size = patch_size\n    # assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n    #     f\"img_size {img_size} should be divided by patch_size {patch_size}.\"\n    self.H, self.W = img_size[0] * patch_size[0], img_size[1] * patch_size[1] # H,W: number of patches in vertical and horizontal directions respectively as a 2D map -- Considering each patch as a pixel, H and W makes sense\n    # self.num_patches = self.H * self.W\n    self.proj = nn.ConvTranspose2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) # Nonoverlapping convolutions to embed each patch\n    self.norm = nn.LayerNorm(embed_dim)\n\n  def forward(self, x) -> Tuple[torch.Tensor, Size_]:\n    B, C, H, W = x.shape # Apparently first input is an image with dimensions: B, C, orig_H, orig_W\n\n    x = self.proj(x).flatten(2).transpose(1, 2) # x: B, H'*W', embed_dim -- H' and W' are the new image where every pixel is actually a patch with 768 dimensions\n    x = self.norm(x)\n    out_size = (H * self.patch_size[0], W * self.patch_size[1])\n\n    return x, out_size # Return both the patch embedded x and its new size\n\nclass Inv_Twins(nn.Module):\n    \"\"\" Twins Vision Transfomer (Revisiting Spatial Attention)\n    Adapted from PVT (PyramidVisionTransformer) class at https://github.com/whai362/PVT.git\n    \"\"\"\n    def __init__(\n            self, img_size=2, patch_size=2, in_chans=512, num_classes=1000, global_pool='avg',\n            embed_dims=(512, 256, 128, 64), num_heads=(1,2,4,8), mlp_ratios=(4, 4, 4, 4), depths=(3, 4, 6, 3),\n            sr_ratios=(1,2,4,8), wss=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n            norm_layer=partial(nn.LayerNorm, eps=1e-6), block_cls=Block):\n        super().__init__()\n        self.num_classes = num_classes # For image classification\n        self.global_pool = global_pool\n        self.depths = depths\n        self.embed_dims = embed_dims\n        self.num_features = embed_dims[-1]\n        self.grad_checkpointing = False\n\n        img_size = to_2tuple(img_size)\n        prev_chs = in_chans # original image channels\n        self.patch_embeds = nn.ModuleList()\n        self.pos_drops = nn.ModuleList() # For dropout\n        for i in range(len(depths)):\n            self.patch_embeds.append(Inv_PatchEmbed(img_size, patch_size, prev_chs, embed_dims[i])) # Initilialize the PatchEmbed modules with different embed_dim for each layer\n            self.pos_drops.append(nn.Dropout(p=drop_rate))\n            prev_chs = embed_dims[i] # Update the prev_chs so we can use PatchEmbed modules\n            img_size = tuple(t * patch_size for t in img_size) # Gradual patching, declaring the new img sizes while passing through each layer\n            patch_size = 2\n\n        self.blocks = nn.ModuleList()\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n        cur = 0\n        for k in range(len(depths)):\n            _block = nn.ModuleList([block_cls(\n                dim=embed_dims[k], num_heads=num_heads[k], mlp_ratio=mlp_ratios[k], drop=drop_rate,\n                attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer, sr_ratio=sr_ratios[k],\n                ws=1 if wss is None or i % 2 == 1 else wss[k]) for i in range(depths[k])]) # In a layer with depth 3 for example the i=0 th layer will do LSA and i = 1 will do GloalSubsampled\n            self.blocks.append(_block)\n            cur += depths[k]\n\n        self.pos_block = nn.ModuleList([Inv_PosConv(embed_dim, embed_dim) for embed_dim in embed_dims])\n\n        self.norm = norm_layer(self.num_features)\n        self.proj = nn.Linear(embed_dims[-1],3)\n        self.act = nn.Sigmoid()\n\n    def forward(self, x):\n        B = x.shape[0] # x: B,H,W,C\n        for i, (inv_embed, drop, blocks, pos_blk) in enumerate(\n                zip(self.patch_embeds, self.pos_drops, self.blocks, self.pos_block)):\n            x, size = inv_embed(x) # First create and embed the patches -- B,H*2,W*2, embed_dim[0] ->\n            # print(size)\n            x = drop(x)\n            for j, blk in enumerate(blocks): # Pass thorugh attention modules but after the first layer add PEG \n                x = blk(x, size)  # x: B,H*2,W*2,embed_dim[0]\n                if j == 0:\n                    x = pos_blk(x, size)  # x: B,H*2,W*2,embed_dim[0]\n            if i < len(self.depths) - 1: # Reshape the x to B, C, H, W unless it is the final output tensor, the final x is x: B, last_H*last_W, last_embedding_dim\n                x = x.reshape(B, *size, -1).permute(0, 3, 1, 2).contiguous()\n        x = self.norm(x)\n        x = x.reshape(B, *size, -1).contiguous()\n        x = self.proj(x)\n        x = self.act(x)\n        x = x.reshape(B,-1,*size)\n\n        return x","metadata":{"_uuid":"7f19ec5e-ab98-4264-ab8b-7fa9c866f102","_cell_guid":"66ce620b-d92a-4f36-95b4-071731b056d0","collapsed":false,"id":"3iWlBGVZidct","execution":{"iopub.status.busy":"2022-08-08T19:36:04.738166Z","iopub.execute_input":"2022-08-08T19:36:04.739144Z","iopub.status.idle":"2022-08-08T19:36:04.769447Z","shell.execute_reply.started":"2022-08-08T19:36:04.739105Z","shell.execute_reply":"2022-08-08T19:36:04.768338Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MergedAutoEncoder(nn.Module):\n    def __init__(self):\n        super(MergedAutoEncoder,self).__init__()\n        ########## encoder #########\n        self.encoder = Twins(img_size = 32, patch_size = 2, in_chans = 3, embed_dims = (64,128,256,512), num_heads = (1,2,4,8), mlp_ratios = (4,4,4,4),\n                            depths = (3,4,6,3),sr_ratios = (8,4,2,1), wss = (2,2,2,2))\n        self.decoder = Inv_Twins(img_size = 2, patch_size = 2,in_chans =512,embed_dims=(512, 256, 128, 64), num_heads=(1,2,4,8), mlp_ratios=(4, 4, 4, 4), depths=(3, 4, 6, 3), sr_ratios=(1,2,4,8), wss=(2,2,2,2))\n    \n    def encode(self,x):\n            x = self.encoder(x)\n            return x\n    \n    def decode(self,x):\n      x = self.decoder(x)\n      return x\n        \n    def forward(self,x):\n            x = self.encode(x)\n            x = self.decode(x)\n            return x","metadata":{"_uuid":"0b618569-d4f1-427a-a8f1-21b034b2d791","_cell_guid":"90d298b4-c3bf-4f94-b16c-70ed6a1e50ee","collapsed":false,"id":"dc5by9JPh6s1","execution":{"iopub.status.busy":"2022-08-08T19:36:04.772280Z","iopub.execute_input":"2022-08-08T19:36:04.772689Z","iopub.status.idle":"2022-08-08T19:36:04.785791Z","shell.execute_reply.started":"2022-08-08T19:36:04.772655Z","shell.execute_reply":"2022-08-08T19:36:04.784650Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs= torch.rand(5,3,32,32)\nautoencoder = MergedAutoEncoder()\nencoder_out = autoencoder.encode(imgs)\nprint(encoder_out.shape)\nx = autoencoder(imgs)\nprint(x.shape)","metadata":{"_uuid":"7404e8bf-7ace-4852-91b3-a3ea7017a3a1","_cell_guid":"d770e1ef-f71a-4c7d-8963-257ce95f4bac","collapsed":false,"id":"ZKnaPEe7h6s2","outputId":"a115c7f6-8d57-4bf9-c3f9-5ffa18cf9e96","execution":{"iopub.status.busy":"2022-08-08T19:36:04.788175Z","iopub.execute_input":"2022-08-08T19:36:04.789106Z","iopub.status.idle":"2022-08-08T19:36:06.144868Z","shell.execute_reply.started":"2022-08-08T19:36:04.789072Z","shell.execute_reply":"2022-08-08T19:36:06.143521Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn.modules import loss\nfrom torchvision.transforms.transforms import RandomVerticalFlip\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.optim.lr_scheduler\nfrom tqdm import tqdm\nimport math\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_transform = transforms.Compose(\n    [\n#      transforms.RandomHorizontalFlip(p=0.5),\n#      transforms.RandomCrop(32,padding=4),\n#      transforms.RandomVerticalFlip(p=0.5),\n     transforms.ToTensor(),\n#      transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n    ])# normalize the image between [-1 1]\n\ntest_transform = transforms.Compose(\n    [transforms.ToTensor(),\n#      transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n    ])\n\nbatch_size = 128\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=train_transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=test_transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\n\nmodel = MergedAutoEncoder().to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(),lr = 1e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='max',patience=2,verbose=True,factor = 0.80)\n\ndef compute_psnr(img1, img2):\n    img1 = img1.astype(np.float64) \n    img2 = img2.astype(np.float64) \n    mse = np.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return \"Same Image\"\n    return 10 * math.log10(1. / mse)\n\n######## Training #########\ndef train(dataloader,model,loss_fn,optimizer):\n  size = len(dataloader.dataset)\n  model.train()\n  for batch, (X,y) in enumerate(dataloader):\n    X = X.to(device) \n    pred = model(X)\n    loss = criterion(pred,X) # The difference between X and  the prediction by model\n    with torch.autograd.set_detect_anomaly(False):\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n      if batch % 100 == 0:\n        loss,current = loss.item(), batch * len(X)\n        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\ndef test(dataloader,model,loss_fn):\n  size = len(dataloader.dataset)\n  num_batches = len(dataloader)\n  model.eval()\n  test_loss, correct = 0,0\n  psnr = 0\n  with torch.no_grad():\n    for X,y in dataloader:\n      X = X.to(device)\n      pred = model(X)\n      psnr += compute_psnr(pred.cpu().numpy(),X.cpu().numpy())\n      test_loss += criterion(pred,X).item()\n    print(f\"PSNR: {psnr/num_batches}\")\n    print(f\"Test Loss: {test_loss/num_batches}\")\n  return psnr/num_batches\n\nepochs = 60\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(trainloader, model, criterion, optimizer)\n    PSNR = test(testloader, model, criterion)\n    scheduler.step(PSNR)\n    # print(f\"The last LR is {scheduler.get_last_lr()[0]}\")\nprint(\"Done!\")","metadata":{"_uuid":"45ccad17-3ba3-4560-ae06-acd3e7d21ea2","_cell_guid":"50f341f5-9405-4b85-b4c4-f621d71f33f9","collapsed":false,"id":"kiM5JxfHiSL3","outputId":"ef19189d-e66a-4ad5-8789-f07039773375","execution":{"iopub.status.busy":"2022-08-08T19:36:06.149279Z","iopub.execute_input":"2022-08-08T19:36:06.149671Z","iopub.status.idle":"2022-08-08T22:47:47.925298Z","shell.execute_reply.started":"2022-08-08T19:36:06.149631Z","shell.execute_reply":"2022-08-08T22:47:47.924016Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Save model\n# torch.save(model.state_dict(), \"model-Twins_Autoencoder_sigmoid.pth\")\n# print(\"Saved PyTorch Model State to model-Twins_Autoencoder_sigmoid.pth\")\n# from google.colab import files\n# files.download( \"model-Twins_Autoencoder.pth\" )","metadata":{"_uuid":"e8a57815-46fd-478e-8752-3e14f8350b56","_cell_guid":"f5dd57cf-38c8-45bd-9fd7-22c3181abeb3","collapsed":false,"id":"i9aE5EDvD2T9","execution":{"iopub.status.busy":"2022-08-08T22:47:47.927696Z","iopub.execute_input":"2022-08-08T22:47:47.928215Z","iopub.status.idle":"2022-08-08T22:47:48.191961Z","shell.execute_reply.started":"2022-08-08T22:47:47.928165Z","shell.execute_reply":"2022-08-08T22:47:48.190885Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfor test_images, _ in testloader:\n    sample_image = test_images[2]\n    break;\nplt.imshow(sample_image.permute(1,2,0))","metadata":{"_uuid":"f1afef46-d3ca-4b07-bcfe-193aef7b3273","_cell_guid":"724b0cb2-a5f5-4ea6-808c-8fc223c4c9db","collapsed":false,"id":"nPBdGhM4DqQg","execution":{"iopub.status.busy":"2022-08-08T22:51:21.452009Z","iopub.execute_input":"2022-08-08T22:51:21.452418Z","iopub.status.idle":"2022-08-08T22:51:21.851888Z","shell.execute_reply.started":"2022-08-08T22:51:21.452384Z","shell.execute_reply":"2022-08-08T22:51:21.850824Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    prediction = model(sample_image.unsqueeze(0).to(device))\n    plt.imshow(prediction.cpu().reshape(3,32,32).permute(1,2,0))","metadata":{"_uuid":"a39ff822-7e1b-47d2-9100-ff3961733f34","_cell_guid":"86130a2a-1406-4785-a12c-dc739770b941","collapsed":false,"execution":{"iopub.status.busy":"2022-08-08T22:51:23.100973Z","iopub.execute_input":"2022-08-08T22:51:23.101617Z","iopub.status.idle":"2022-08-08T22:51:23.305301Z","shell.execute_reply.started":"2022-08-08T22:51:23.101579Z","shell.execute_reply":"2022-08-08T22:51:23.304420Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}