{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NormalConvs-CoatDJSCC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cloning coatnet.py and importing modules"
      ],
      "metadata": {
        "id": "bhqaMz7iWFIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKFI-wMRYTyO",
        "outputId": "26f0c889-0273-44f0-d5d9-2daa21ebffac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "\n",
        "def conv_3x3_bn(inp, oup, image_size, downsample=False):\n",
        "    stride = 1 if downsample == False else 2\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.GELU()\n",
        "    )\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn, norm):\n",
        "        super().__init__()\n",
        "        self.norm = norm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class SE(nn.Module):\n",
        "    def __init__(self, inp, oup, expansion=0.25):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n",
        "        super().__init__()\n",
        "        self.downsample = downsample\n",
        "        stride = 1 if self.downsample == False else 2\n",
        "        hidden_dim = int(inp * expansion)\n",
        "\n",
        "        if self.downsample:\n",
        "            self.pool = nn.MaxPool2d(3, 2, 1)\n",
        "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "\n",
        "        if expansion == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
        "                          1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                # down-sample in the first conv\n",
        "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
        "                          groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                SE(inp, hidden_dim),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        \n",
        "        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.downsample:\n",
        "            return self.proj(self.pool(x)) + self.conv(x)\n",
        "        else:\n",
        "            return x + self.conv(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        project_out = not (heads == 1 and dim_head == inp)\n",
        "\n",
        "        self.ih, self.iw = image_size\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        # parameter table of relative position bias\n",
        "        self.relative_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
        "\n",
        "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
        "        coords = torch.flatten(torch.stack(coords), 1)\n",
        "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
        "\n",
        "        relative_coords[0] += self.ih - 1\n",
        "        relative_coords[1] += self.iw - 1\n",
        "        relative_coords[0] *= 2 * self.iw - 1\n",
        "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
        "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
        "        self.register_buffer(\"relative_index\", relative_index)\n",
        "\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, oup),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(\n",
        "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        # Use \"gather\" for more efficiency on GPUs\n",
        "        relative_bias = self.relative_bias_table.gather(\n",
        "            0, self.relative_index.repeat(1, self.heads))\n",
        "        relative_bias = rearrange(\n",
        "            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
        "        dots = dots + relative_bias\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(inp * 4)\n",
        "\n",
        "        self.ih, self.iw = image_size\n",
        "        self.downsample = downsample\n",
        "\n",
        "        if self.downsample:\n",
        "            self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
        "            self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
        "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "\n",
        "        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n",
        "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
        "\n",
        "        self.attn = nn.Sequential(\n",
        "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
        "            PreNorm(inp, self.attn, nn.LayerNorm),\n",
        "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "        )\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
        "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
        "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.downsample:\n",
        "            x = self.proj(self.pool1(x)) + self.attn(self.pool2(x))\n",
        "        else:\n",
        "            x = x + self.attn(x)\n",
        "        x = x + self.ff(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CoAtNet(nn.Module):\n",
        "    def __init__(self, image_size, in_channels, num_blocks, channels, num_classes=1000, block_types=['C', 'C', 'T', 'T']):\n",
        "        super().__init__()\n",
        "        ih, iw = image_size\n",
        "        block = {'C': MBConv, 'T': Transformer}\n",
        "\n",
        "        self.s0 = self._make_layer(\n",
        "            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n",
        "        self.s1 = self._make_layer(\n",
        "            block[block_types[0]], channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n",
        "        self.s2 = self._make_layer(\n",
        "            block[block_types[1]], channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n",
        "        self.s3 = self._make_layer(\n",
        "            block[block_types[2]], channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n",
        "        self.s4 = self._make_layer(\n",
        "            block[block_types[3]], channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n",
        "\n",
        "        self.pool = nn.AvgPool2d(ih // 32, 1)\n",
        "        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.s0(x)\n",
        "        x = self.s1(x)\n",
        "        x = self.s2(x)\n",
        "        x = self.s3(x)\n",
        "        x = self.s4(x)\n",
        "\n",
        "        x = self.pool(x).view(-1, x.shape[1])\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, inp, oup, depth, image_size):\n",
        "        layers = nn.ModuleList([])\n",
        "        for i in range(depth):\n",
        "            if i == 0:\n",
        "                layers.append(block(inp, oup, image_size, downsample=True))\n",
        "            else:\n",
        "                layers.append(block(oup, oup, image_size))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def coatnet_0():\n",
        "    num_blocks = [2, 2, 3, 5, 2]            # L\n",
        "    channels = [64, 96, 192, 384, 768]      # D\n",
        "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def coatnet_1():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [64, 96, 192, 384, 768]      # D\n",
        "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def coatnet_2():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [128, 128, 256, 512, 1026]   # D\n",
        "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def coatnet_3():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [192, 192, 384, 768, 1536]   # D\n",
        "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def coatnet_4():\n",
        "    num_blocks = [2, 2, 12, 28, 2]          # L\n",
        "    channels = [192, 192, 384, 768, 1536]   # D\n",
        "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n"
      ],
      "metadata": {
        "id": "P2aTHM5KWb0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random 3x224x224 images"
      ],
      "metadata": {
        "id": "scZF68EQW1C2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = torch.randn(5,3,32,32)"
      ],
      "metadata": {
        "id": "Z1pLuKE6WeRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s0 = conv_3x3_bn(3,64,(16,16),downsample = True) #imsize is the output size\n",
        "s0_output = s0(images)\n",
        "print(s0_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3SvzRBAXFWp",
        "outputId": "ca87afcc-0546-4871-d258-98c57c7df489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 64, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = MBConv(64,96,(8,8),downsample = True,expansion=4)\n",
        "s1_output = s1(s0_output)\n",
        "print(s1_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TMWb8U_YZz2",
        "outputId": "d0b07c52-8cc3-45de-9ed1-d6caf2fbcc4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 96, 8, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s2 = MBConv(96,192,(4,4),downsample = True, expansion = 4)\n",
        "s2_output = s2(s1_output)\n",
        "print(s2_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Shs7IseZwIZ",
        "outputId": "bd129de4-6415-4d4f-9799-07cf7500041b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 192, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s3 = Transformer(192,384,(2,2),downsample = True)\n",
        "s3_output = s3(s2_output)\n",
        "print(s3_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1oVa06xZ4e1",
        "outputId": "ea4163df-54a3-423e-949b-cbf27988a6b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 384, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s4 = Transformer(384,768,(1,1),downsample = True)\n",
        "s4_output = s4(s3_output)\n",
        "print(s4_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3_eN1yLjBi0",
        "outputId": "98384891-84e7-4589-cb49-a8824d563c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 768, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "upsamplerrr = nn.ConvTranspose2d(3072,200,2,2)\n",
        "test = upsamplerrr(s4_output)\n",
        "print(test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bTBJ4XtHN-v",
        "outputId": "d15daf53-01e2-4310-dc64-06730fb54839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 200, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s4_output[0,0,:,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiySnFMXjHp6",
        "outputId": "624a1af2-809c-44f5-ad89-068bc9df8e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1653]], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Symmetrical Coatnet"
      ],
      "metadata": {
        "id": "aMlS6LAJmNCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------Same as CoAtNet--------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn, norm):\n",
        "        super().__init__()\n",
        "        self.norm = norm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class SE(nn.Module):\n",
        "    def __init__(self, inp, oup, expansion=0.25):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        project_out = not (heads == 1 and dim_head == inp)\n",
        "\n",
        "        self.ih, self.iw = image_size\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        # parameter table of relative position bias\n",
        "        self.relative_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
        "\n",
        "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
        "        coords = torch.flatten(torch.stack(coords), 1)\n",
        "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
        "\n",
        "        relative_coords[0] += self.ih - 1\n",
        "        relative_coords[1] += self.iw - 1\n",
        "        relative_coords[0] *= 2 * self.iw - 1\n",
        "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
        "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
        "        self.register_buffer(\"relative_index\", relative_index)\n",
        "\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, oup),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(\n",
        "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        # Use \"gather\" for more efficiency on GPUs\n",
        "        relative_bias = self.relative_bias_table.gather(\n",
        "            0, self.relative_index.repeat(1, self.heads))\n",
        "        relative_bias = rearrange(\n",
        "            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
        "        dots = dots + relative_bias\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "# -----------------Symmetric CoAtNet Modules--------------\n",
        "class Bicubic_upsampler(nn.Module):\n",
        "    def __init__(self,scale_factor,mode):\n",
        "      super(Bicubic_upsampler,self).__init__()\n",
        "      self.upsampler = nn.functional.interpolate\n",
        "      self.scale_factor = scale_factor\n",
        "      self.mode = mode\n",
        "    def forward(self,x):\n",
        "      x = self.upsampler(x,scale_factor = self.scale_factor, mode = self.mode)\n",
        "      return x\n",
        "\n",
        "\n",
        "class InverseTransformer(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, upsample=False, dropout=0.):\n",
        "        super().__init__()\n",
        "        \n",
        "\n",
        "        self.ih, self.iw = image_size\n",
        "        self.upsample = upsample\n",
        "        \n",
        "        \n",
        "        if self.upsample: #We can change the upsampling method at some point.\n",
        "          #Maybe using bicubic interpolation might be better\n",
        "          \n",
        "          # self.upsampler = nn.PixelShuffle(2) # 2 is the upsample factor can be a hyperparameter\n",
        "          # inp = int(inp/4) # after upsampling \n",
        "          self.upsampler = nn.ConvTranspose2d(inp,inp,kernel_size=2,stride=2,bias=False) \n",
        "\n",
        "          # Not needed since pixelshuffle reduces channels while upsampling, Needed when using bicubic interpolation\n",
        "          # self.upsampler = Bicubic_upsampler(scale_factor = 2, mode = \"bicubic\")\n",
        "          self.proj = nn.ConvTranspose2d(inp, oup, 1, 1, 0, bias=False) \n",
        "\n",
        "        hidden_dim = int(inp * 4)\n",
        "        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n",
        "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
        "\n",
        "        self.attn = nn.Sequential(\n",
        "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
        "            PreNorm(inp, self.attn, nn.LayerNorm),\n",
        "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "        )\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
        "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
        "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample:\n",
        "            x = self.proj(self.upsampler(x)) + self.attn(self.upsampler(x))\n",
        "        else:\n",
        "            x = x + self.attn(x)\n",
        "        x = x + self.ff(x)\n",
        "        return x\n",
        "\n",
        "class InverseMBConv(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, upsample=False, expansion=4):\n",
        "        super().__init__()\n",
        "        self.upsample = upsample\n",
        "        stride = 1 if self.upsample == False else 2\n",
        "        output_padding = 0 if self.upsample == False else 1\n",
        "\n",
        "        if self.upsample:\n",
        "            # self.upsampler = nn.PixelShuffle(2)\n",
        "            # self.upsampler = Bicubic_upsampler(scale_factor = 2, mode = \"bicubic\")\n",
        "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "            self.upsampler = nn.ConvTranspose2d(inp,inp,kernel_size=2,stride=2,bias=False)\n",
        "        \n",
        "        hidden_dim = int(inp * expansion)\n",
        "      \n",
        "        if expansion == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(inp, hidden_dim, 3, stride,\n",
        "                          1, groups=inp, bias=False,output_padding = output_padding),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                # up-sample in the first conv\n",
        "                nn.ConvTranspose2d(inp, hidden_dim, 1, stride, 0, bias=False,output_padding = output_padding),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
        "                          groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                SE(inp, hidden_dim),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        \n",
        "        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample:\n",
        "            return self.proj(self.upsampler(x)) + self.conv(x)\n",
        "        else:\n",
        "            return x + self.conv(x)\n",
        "\n",
        "def Inverse_conv_3x3_bn(inp, oup, image_size, upsample=False):\n",
        "    stride = 1 if upsample == False else 2\n",
        "    output_padding = 0 if upsample == False else 1\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(inp, oup, 3, stride, 1, bias=False,output_padding = output_padding),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.Tanh()\n",
        "    )"
      ],
      "metadata": {
        "id": "2ngNf3yTm3l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recreating the Image"
      ],
      "metadata": {
        "id": "2eldj5XvMWmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g4 = InverseTransformer(768,384,(2,2),upsample = True)\n",
        "g4_output = g4(s4_output)\n",
        "print(g4_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWFvcuhz-A8q",
        "outputId": "dcba1a08-50c9-4c61-bd0c-ba4261c446fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 384, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g3 = InverseTransformer(384,192,(4,4),upsample = True)\n",
        "g3_output = g3(g4_output)\n",
        "print(g3_output.shape)"
      ],
      "metadata": {
        "id": "ULmmBESw_C9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37d28db-91f4-40c2-c89e-fc3fdce98307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 192, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g2 = InverseMBConv(192,96,(8,8),upsample = True)\n",
        "g2_output = g2(g3_output)\n",
        "print(g2_output.shape)"
      ],
      "metadata": {
        "id": "RipHJjQtBG2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c58a86-2428-4351-e395-66a8983d0736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 96, 8, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g1 = InverseMBConv(96,64,(16,16),upsample = True)\n",
        "g1_output = g1(g2_output)\n",
        "print(g1_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7sC2cEEBZbb",
        "outputId": "603a9b86-0f46-4d89-fdbf-3c60721fec3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 64, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g0 = Inverse_conv_3x3_bn(64,3,(32,32),upsample = True)\n",
        "g0_output = g0(g1_output)\n",
        "print(g0_output.shape)"
      ],
      "metadata": {
        "id": "8SPUadwWBkPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92737496-2c01-4799-dfee-22b86ffb9aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lolconv = nn.ConvTranspose2d(192,768,1,2,0,bias=False, output_padding = 1)\n",
        "test = lolconv(g3_output)\n",
        "print(test.shape)\n",
        "testconv = nn.ConvTranspose2d(768,768,3,1,1,groups = 768,bias = False)\n",
        "test = testconv(test)\n",
        "print(test.shape)\n",
        "se = SE(192,768)\n",
        "test = se(test)\n",
        "print(test.shape)\n",
        "pwconv = nn.ConvTranspose2d(768,48,1,1,0,bias = False)\n",
        "test = pwconv(test)\n",
        "print(test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1LHCxOK-Qa9",
        "outputId": "cb0da218-2928-4471-edc0-d694c858cdaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 768, 56, 56])\n",
            "torch.Size([1, 768, 56, 56])\n",
            "torch.Size([1, 768, 56, 56])\n",
            "torch.Size([1, 48, 56, 56])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merged Auto Encoder"
      ],
      "metadata": {
        "id": "80XoOA0xMVbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class MLP(nn.Module):\n",
        "#   def __init__(self,inp,oup,dropout = 0.):\n",
        "#     super().__init__()\n",
        "#     hidden = int(inp/2)\n",
        "#     self.fc1 = nn.Linear(inp,hidden)\n",
        "#     self.act = nn.GELU()\n",
        "#     self.fc2 = nn.Linear(hidden,oup)\n",
        "#     self.drop = nn.Dropout(dropout)\n",
        "#     self.norm_layer1 = nn.LayerNorm(hidden)\n",
        "#     self.norm_layer2 = nn.LayerNorm(oup)\n",
        "#   def forward(self,x):\n",
        "#     x = rearrange(x,'b c h w -> b h w c')\n",
        "#     x = self.fc1(x)\n",
        "#     # x = self.norm_layer1(x)\n",
        "#     x = self.act(x)\n",
        "#     x = self.drop(x)\n",
        "#     x = self.fc2(x)\n",
        "#     # x = self.norm_layer2(x)\n",
        "#     x = self.drop(x)\n",
        "#     x = rearrange(x,'b h w c -> b c h w')\n",
        "#     return x\n",
        "\n",
        "# class Inverse_MLP(nn.Module):\n",
        "#   def __init__(self,inp,oup,dropout = 0.):\n",
        "#     super().__init__()\n",
        "#     hidden = int(inp*2)\n",
        "#     self.fc1 = nn.Linear(inp,hidden)\n",
        "#     self.act = nn.GELU()\n",
        "#     self.fc2 = nn.Linear(hidden,oup)\n",
        "#     self.drop = nn.Dropout(dropout)\n",
        "#     self.norm_layer1 = nn.LayerNorm(hidden)\n",
        "#     self.norm_layer2 = nn.LayerNorm(oup)\n",
        "#   def forward(self,x):\n",
        "#     x = rearrange(x,'b c h w -> b h w c')\n",
        "#     x = self.fc1(x)\n",
        "#     # x = self.norm_layer1(x)\n",
        "#     x = self.act(x)\n",
        "#     x = self.drop(x)\n",
        "#     x = self.fc2(x)\n",
        "#     # x = self.norm_layer2(x)\n",
        "#     x = self.drop(x)\n",
        "#     x = rearrange(x,'b h w c -> b c h w')\n",
        "#     return x\n",
        "\n",
        "class MergedAutoEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MergedAutoEncoder,self).__init__()\n",
        "    ######### encoder layers #########\n",
        "    #self.a0 =  conv_3x3_bn(3,12,(16,16),downsample = True)\n",
        "    self.a0 = self._make_layer_analysis(conv_3x3_bn,3,128,1,(16,16))\n",
        "    # self.gdn12 = GDN(12)\n",
        "\n",
        "    #self.a1 = MBConv(12,48,(8,8),downsample = True)\n",
        "    self.a1 = self._make_layer_analysis(MBConv,128,128,2,(8,8))\n",
        "    # self.gdn48 = GDN(48)\n",
        "\n",
        "    #self.a2 = MBConv(48,192,(4,4),downsample = True)\n",
        "    self.a2 = self._make_layer_analysis(MBConv,128,256,6,(4,4))\n",
        "    # self.gdn192 = GDN(192)\n",
        "\n",
        "    #self.a3 = Transformer(192,768,(2,2),downsample=True)\n",
        "    self.a3 = self._make_layer_analysis(Transformer,256,512,14,(2,2))\n",
        "    # self.gdn768 = GDN(768)\n",
        "\n",
        "    #self.a4 = Transformer(768,3072,(1,1),downsample = True)\n",
        "    self.a4 = self._make_layer_analysis(Transformer,512,1024,2,(1,1))\n",
        "    # self.gdn3072 = GDN(3072)\n",
        "\n",
        "    # self.compress = MLP(3072,192)\n",
        "\n",
        "    ######### decoder layers #########\n",
        "\n",
        "    # self. decompress = Inverse_MLP(192,3072)\n",
        "\n",
        "    #self.s4 = InverseTransformer(3072,768,(2,2),upsample = True)\n",
        "    self.s4 = self._make_layer_synthesis(InverseTransformer,1024,512,2,(2,2))\n",
        "    # self.igdn768 = GDN(768,inverse=True)\n",
        "\n",
        "    #self.s3 = InverseTransformer(768,192,(4,4),upsample = True)\n",
        "    self.s3 = self._make_layer_synthesis(InverseTransformer,512,256,14,(4,4))\n",
        "    # self.igdn192 = GDN(192,inverse=True)\n",
        "\n",
        "    # self.s2 = InverseMBConv(192,48,(8,8),upsample= True)\n",
        "    self.s2 = self._make_layer_synthesis(InverseMBConv,256,128,6,(8,8))\n",
        "\n",
        "    # self.igdn48= GDN(48,inverse=True)\n",
        "\n",
        "    # self.s1 = InverseMBConv(48,12,(16,16),upsample = True)\n",
        "    self.s1 = self._make_layer_synthesis(InverseMBConv,128,128,2,(16,16))\n",
        "    # self.igdn12= GDN(12,inverse=True)\n",
        "\n",
        "    # self.s0 = Inverse_conv_3x3_bn(12,3,(32,32),upsample = True)\n",
        "    self.s0 = self._make_layer_synthesis(Inverse_conv_3x3_bn,128,3,1,(32,32))\n",
        "    # self.igdn3= GDN(3,inverse=True)\n",
        "\n",
        "  def encode(self,x):\n",
        "    x = self.a0(x)\n",
        "    x = self.a1(x)\n",
        "    x = self.a2(x)\n",
        "    x = self.a3(x)\n",
        "    x = self.a4(x)\n",
        "    # x = self.compress(x)\n",
        "    return x\n",
        "\n",
        "  def decode(self,x):\n",
        "    # x = self.decompress(x)\n",
        "    x = self.s4(x)\n",
        "    x = self.s3(x)\n",
        "    x = self.s2(x)\n",
        "    x = self.s1(x)\n",
        "    x = self.s0(x)\n",
        "    return x\n",
        "\n",
        "  def forward(self,x):\n",
        "    enc = self.encode(x)\n",
        "    x_hat = self.decode(enc)\n",
        "    return x_hat\n",
        "\n",
        "  def _make_layer_analysis(self,block,inp,oup,depth,image_size):\n",
        "    layers = nn.ModuleList([])\n",
        "    for i in range(depth):\n",
        "      if i == 0:\n",
        "        layers.append(block(inp,oup,image_size,downsample = True))\n",
        "      else:\n",
        "        layers.append(block(oup,oup,image_size))\n",
        "    return nn.Sequential(*layers)\n",
        "  \n",
        "  def _make_layer_synthesis(self,block,inp,oup,depth,image_size):\n",
        "    layers = nn.ModuleList([])\n",
        "    for i in range(depth):\n",
        "      if i == 0:\n",
        "        layers.append(block(inp,oup,image_size,upsample = True))\n",
        "      else:\n",
        "        layers.append(block(oup,oup,image_size))\n",
        "    return nn.Sequential(*layers)\n"
      ],
      "metadata": {
        "id": "A2Y-1kwcMyku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main"
      ],
      "metadata": {
        "id": "cbeQV_IoRS_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import loss\n",
        "from torchvision.transforms.transforms import RandomVerticalFlip\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "      # transforms.RandomHorizontalFlip(p=0.5),\n",
        "    #  transforms.RandomCrop(32,padding=4),\n",
        "    #  transforms.RandomVerticalFlip(p=0.5),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
        "    ])# normalize the image between [-1 1]\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
        "    ])\n",
        "\n",
        "batch_size = 128\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "model = MergedAutoEncoder().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 2e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',patience=2,verbose=True)\n",
        "\n",
        "def compute_psnr(img1, img2):\n",
        "    img1 = img1.astype(np.float64) \n",
        "    img2 = img2.astype(np.float64) \n",
        "    mse = np.mean((img1 - img2) ** 2)\n",
        "    if mse == 0:\n",
        "        return \"Same Image\"\n",
        "    return 10 * math.log10(1. / mse)\n",
        "\n",
        "######## Training #########\n",
        "def train(dataloader,model,loss_fn,optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "    X,y = X.to(device), y.to(device) # I guess we dont actually need the labels\n",
        "    pred = model(X)\n",
        "    loss = criterion(pred,X) # The difference between X and  the prediction by model\n",
        "    with torch.autograd.set_detect_anomaly(False):\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if batch % 100 == 0:\n",
        "        loss,current = loss.item(), batch * len(X)\n",
        "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "  return loss.item()\n",
        "\n",
        "def test(dataloader,model,loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0,0\n",
        "  psnr = 0\n",
        "  with torch.no_grad():\n",
        "    for X,y in dataloader:\n",
        "      X,y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      psnr += compute_psnr(pred.cpu().numpy(),X.cpu().numpy())\n",
        "      test_loss += criterion(pred,X).item()\n",
        "    print(f\"PSNR: {psnr/num_batches}\")\n",
        "    print(f\"Test Loss: {test_loss/num_batches}\")\n",
        "\n",
        "epochs = 60\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss = train(trainloader, model, criterion, optimizer)\n",
        "    test(testloader, model, criterion)\n",
        "    scheduler.step(train_loss)\n",
        "    # print(f\"The last LR is {scheduler.get_last_lr()[0]}\")\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2j1S9N8RT7J",
        "outputId": "6bdb93f1-5a5b-4979-a4e8-afb25fc574ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.593021  [    0/50000]\n",
            "loss: 0.150868  [12800/50000]\n",
            "loss: 0.157247  [25600/50000]\n",
            "loss: 0.127978  [38400/50000]\n",
            "PSNR: 9.498923439820178\n",
            "Test Loss: 0.11235343674315681\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.109529  [    0/50000]\n",
            "loss: 0.108629  [12800/50000]\n",
            "loss: 0.098070  [25600/50000]\n",
            "loss: 0.092001  [38400/50000]\n",
            "PSNR: 10.836457425120754\n",
            "Test Loss: 0.08258123035672345\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.081467  [    0/50000]\n",
            "loss: 0.082716  [12800/50000]\n",
            "loss: 0.070842  [25600/50000]\n",
            "loss: 0.069533  [38400/50000]\n",
            "PSNR: 11.843284947696434\n",
            "Test Loss: 0.06549550121343589\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.068221  [    0/50000]\n",
            "loss: 0.069345  [12800/50000]\n",
            "loss: 0.058507  [25600/50000]\n",
            "loss: 0.057883  [38400/50000]\n",
            "PSNR: 12.42751452425399\n",
            "Test Loss: 0.05724887521583823\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.058480  [    0/50000]\n",
            "loss: 0.054210  [12800/50000]\n",
            "loss: 0.055656  [25600/50000]\n",
            "loss: 0.051591  [38400/50000]\n",
            "PSNR: 12.983065781282805\n",
            "Test Loss: 0.05037145821165435\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.051501  [    0/50000]\n",
            "loss: 0.049345  [12800/50000]\n",
            "loss: 0.047846  [25600/50000]\n",
            "loss: 0.045590  [38400/50000]\n",
            "PSNR: 13.457846622201757\n",
            "Test Loss: 0.04515114412466182\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.043809  [    0/50000]\n",
            "loss: 0.042488  [12800/50000]\n",
            "loss: 0.043857  [25600/50000]\n",
            "loss: 0.044709  [38400/50000]\n",
            "PSNR: 13.813953461977606\n",
            "Test Loss: 0.041596094217104244\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.040899  [    0/50000]\n",
            "loss: 0.041000  [12800/50000]\n",
            "loss: 0.041702  [25600/50000]\n",
            "loss: 0.041721  [38400/50000]\n",
            "PSNR: 14.104729852113538\n",
            "Test Loss: 0.03890249482061289\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.038057  [    0/50000]\n",
            "loss: 0.041456  [12800/50000]\n",
            "loss: 0.040246  [25600/50000]\n",
            "loss: 0.044839  [38400/50000]\n",
            "PSNR: 14.36409409838205\n",
            "Test Loss: 0.03664457312301744\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.037438  [    0/50000]\n",
            "loss: 0.035082  [12800/50000]\n",
            "loss: 0.035706  [25600/50000]\n",
            "loss: 0.036797  [38400/50000]\n",
            "PSNR: 14.722192611897226\n",
            "Test Loss: 0.03374528844805458\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.036871  [    0/50000]\n",
            "loss: 0.033645  [12800/50000]\n",
            "loss: 0.036630  [25600/50000]\n",
            "loss: 0.032300  [38400/50000]\n",
            "PSNR: 15.064675343325002\n",
            "Test Loss: 0.031186862506821185\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.030494  [    0/50000]\n",
            "loss: 0.029392  [12800/50000]\n",
            "loss: 0.033490  [25600/50000]\n",
            "loss: 0.030101  [38400/50000]\n",
            "PSNR: 15.261369426136016\n",
            "Test Loss: 0.029805450664856768\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.027544  [    0/50000]\n",
            "loss: 0.028580  [12800/50000]\n",
            "loss: 0.029476  [25600/50000]\n",
            "loss: 0.027956  [38400/50000]\n",
            "PSNR: 15.46946647554409\n",
            "Test Loss: 0.028410668260877646\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.030037  [    0/50000]\n",
            "loss: 0.028521  [12800/50000]\n",
            "loss: 0.027077  [25600/50000]\n",
            "loss: 0.026757  [38400/50000]\n",
            "PSNR: 15.733944143619832\n",
            "Test Loss: 0.026732666087867337\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.027359  [    0/50000]\n",
            "loss: 0.026060  [12800/50000]\n",
            "loss: 0.026940  [25600/50000]\n",
            "loss: 0.026533  [38400/50000]\n",
            "PSNR: 15.893729848101323\n",
            "Test Loss: 0.02576699225774294\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.025410  [    0/50000]\n",
            "loss: 0.026336  [12800/50000]\n",
            "loss: 0.025782  [25600/50000]\n",
            "loss: 0.024263  [38400/50000]\n",
            "PSNR: 16.16741151330867\n",
            "Test Loss: 0.02419400913051412\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.024220  [    0/50000]\n",
            "loss: 0.023760  [12800/50000]\n",
            "loss: 0.024453  [25600/50000]\n",
            "loss: 0.025013  [38400/50000]\n",
            "PSNR: 16.206184413158038\n",
            "Test Loss: 0.02397777206158336\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.024096  [    0/50000]\n",
            "loss: 0.025489  [12800/50000]\n",
            "loss: 0.023267  [25600/50000]\n",
            "loss: 0.022692  [38400/50000]\n",
            "PSNR: 16.398173775372523\n",
            "Test Loss: 0.022942411277113082\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.022859  [    0/50000]\n",
            "loss: 0.022422  [12800/50000]\n",
            "loss: 0.027054  [25600/50000]\n",
            "loss: 0.022074  [38400/50000]\n",
            "PSNR: 16.601993925422217\n",
            "Test Loss: 0.021891287610500673\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.022784  [    0/50000]\n",
            "loss: 0.024475  [12800/50000]\n",
            "loss: 0.023948  [25600/50000]\n",
            "loss: 0.022585  [38400/50000]\n",
            "PSNR: 16.652992847046335\n",
            "Test Loss: 0.021634571395720108\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.020862  [    0/50000]\n",
            "loss: 0.022966  [12800/50000]\n",
            "loss: 0.021976  [25600/50000]\n",
            "loss: 0.021966  [38400/50000]\n",
            "PSNR: 16.755319559356657\n",
            "Test Loss: 0.02113103312499161\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.021418  [    0/50000]\n",
            "loss: 0.020425  [12800/50000]\n",
            "loss: 0.020184  [25600/50000]\n",
            "loss: 0.020426  [38400/50000]\n",
            "PSNR: 16.74287103582032\n",
            "Test Loss: 0.021192493765980383\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.021525  [    0/50000]\n",
            "loss: 0.021181  [12800/50000]\n",
            "loss: 0.020641  [25600/50000]\n",
            "loss: 0.020851  [38400/50000]\n",
            "PSNR: 16.731881232904858\n",
            "Test Loss: 0.021244761663713033\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.021020  [    0/50000]\n",
            "loss: 0.019747  [12800/50000]\n",
            "loss: 0.020734  [25600/50000]\n",
            "loss: 0.020566  [38400/50000]\n",
            "PSNR: 17.002888302732543\n",
            "Test Loss: 0.01996054688963709\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.020913  [    0/50000]\n",
            "loss: 0.019036  [12800/50000]\n",
            "loss: 0.018595  [25600/50000]\n",
            "loss: 0.019975  [38400/50000]\n",
            "PSNR: 17.153748905445728\n",
            "Test Loss: 0.019278384980898868\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.019453  [    0/50000]\n",
            "loss: 0.019786  [12800/50000]\n",
            "loss: 0.018449  [25600/50000]\n",
            "loss: 0.020076  [38400/50000]\n",
            "PSNR: 17.24853404845817\n",
            "Test Loss: 0.01886271124210539\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.018265  [    0/50000]\n",
            "loss: 0.018095  [12800/50000]\n",
            "loss: 0.017571  [25600/50000]\n",
            "loss: 0.019258  [38400/50000]\n",
            "PSNR: 17.179375701090475\n",
            "Test Loss: 0.019164764970729622\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.018530  [    0/50000]\n",
            "loss: 0.017751  [12800/50000]\n",
            "loss: 0.019508  [25600/50000]\n",
            "loss: 0.019885  [38400/50000]\n",
            "PSNR: 17.20924151506843\n",
            "Test Loss: 0.01903326342566104\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.020400  [    0/50000]\n",
            "loss: 0.020299  [12800/50000]\n",
            "loss: 0.018213  [25600/50000]\n",
            "loss: 0.021140  [38400/50000]\n",
            "PSNR: 17.370212360960153\n",
            "Test Loss: 0.018341988348696804\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.019250  [    0/50000]\n",
            "loss: 0.020974  [12800/50000]\n",
            "loss: 0.022690  [25600/50000]\n",
            "loss: 0.018303  [38400/50000]\n",
            "PSNR: 17.211471109108643\n",
            "Test Loss: 0.01902411559808858\n",
            "Epoch 00030: reducing learning rate of group 0 to 2.0000e-04.\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.020218  [    0/50000]\n",
            "loss: 0.017525  [12800/50000]\n",
            "loss: 0.018579  [25600/50000]\n",
            "loss: 0.017202  [38400/50000]\n",
            "PSNR: 17.83206595873932\n",
            "Test Loss: 0.016492835493593275\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.016026  [    0/50000]\n",
            "loss: 0.017579  [12800/50000]\n",
            "loss: 0.018184  [25600/50000]\n",
            "loss: 0.015091  [38400/50000]\n",
            "PSNR: 17.869619284601512\n",
            "Test Loss: 0.016350720254586466\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.017988  [    0/50000]\n",
            "loss: 0.017214  [12800/50000]\n",
            "loss: 0.016429  [25600/50000]\n",
            "loss: 0.016684  [38400/50000]\n",
            "PSNR: 17.88087210127449\n",
            "Test Loss: 0.016308400862484793\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.017069  [    0/50000]\n",
            "loss: 0.017422  [12800/50000]\n",
            "loss: 0.017670  [25600/50000]\n",
            "loss: 0.017990  [38400/50000]\n",
            "PSNR: 17.85926573544532\n",
            "Test Loss: 0.01638975672282373\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.016883  [    0/50000]\n",
            "loss: 0.016139  [12800/50000]\n",
            "loss: 0.016165  [25600/50000]\n",
            "loss: 0.017142  [38400/50000]\n",
            "PSNR: 17.884537608046166\n",
            "Test Loss: 0.016294495344067676\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.015497  [    0/50000]\n",
            "loss: 0.016246  [12800/50000]\n",
            "loss: 0.017151  [25600/50000]\n",
            "loss: 0.015572  [38400/50000]\n",
            "PSNR: 17.893326511436793\n",
            "Test Loss: 0.016261584549859355\n",
            "Epoch 00036: reducing learning rate of group 0 to 2.0000e-05.\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.015045  [    0/50000]\n",
            "loss: 0.015549  [12800/50000]\n",
            "loss: 0.016255  [25600/50000]\n",
            "loss: 0.017771  [38400/50000]\n",
            "PSNR: 17.97482877311075\n",
            "Test Loss: 0.01595967726286831\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.016267  [    0/50000]\n",
            "loss: 0.015707  [12800/50000]\n",
            "loss: 0.016199  [25600/50000]\n",
            "loss: 0.016715  [38400/50000]\n",
            "PSNR: 17.984966853372143\n",
            "Test Loss: 0.01592248932847494\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.016115  [    0/50000]\n",
            "loss: 0.013738  [12800/50000]\n",
            "loss: 0.015912  [25600/50000]\n",
            "loss: 0.015640  [38400/50000]\n",
            "PSNR: 17.993454675003868\n",
            "Test Loss: 0.01589146571210291\n",
            "Epoch 00039: reducing learning rate of group 0 to 2.0000e-06.\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.014221  [    0/50000]\n",
            "loss: 0.017559  [12800/50000]\n",
            "loss: 0.015477  [25600/50000]\n",
            "loss: 0.016080  [38400/50000]\n",
            "PSNR: 17.995267944643828\n",
            "Test Loss: 0.015884829637936398\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.016802  [    0/50000]\n",
            "loss: 0.015318  [12800/50000]\n",
            "loss: 0.015567  [25600/50000]\n",
            "loss: 0.017927  [38400/50000]\n",
            "PSNR: 17.983708657328375\n",
            "Test Loss: 0.015927054259124437\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.016253  [    0/50000]\n",
            "loss: 0.016097  [12800/50000]\n",
            "loss: 0.015596  [25600/50000]\n",
            "loss: 0.016477  [38400/50000]\n",
            "PSNR: 17.99531508316191\n",
            "Test Loss: 0.015884655303781546\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.015058  [    0/50000]\n",
            "loss: 0.017043  [12800/50000]\n",
            "loss: 0.015175  [25600/50000]\n",
            "loss: 0.018280  [38400/50000]\n",
            "PSNR: 17.9942009367613\n",
            "Test Loss: 0.015888733836480335\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.015681  [    0/50000]\n",
            "loss: 0.018390  [12800/50000]\n",
            "loss: 0.015264  [25600/50000]\n",
            "loss: 0.016220  [38400/50000]\n",
            "PSNR: 17.994428996667384\n",
            "Test Loss: 0.015887859701827357\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.015430  [    0/50000]\n",
            "loss: 0.015480  [12800/50000]\n",
            "loss: 0.015401  [25600/50000]\n",
            "loss: 0.015646  [38400/50000]\n",
            "PSNR: 17.998035545204203\n",
            "Test Loss: 0.01587472349122355\n",
            "Epoch 00045: reducing learning rate of group 0 to 2.0000e-07.\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.016751  [    0/50000]\n",
            "loss: 0.015552  [12800/50000]\n",
            "loss: 0.017176  [25600/50000]\n",
            "loss: 0.016056  [38400/50000]\n",
            "PSNR: 17.98050973931594\n",
            "Test Loss: 0.015938733232832406\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.016015  [    0/50000]\n",
            "loss: 0.016415  [12800/50000]\n",
            "loss: 0.017557  [25600/50000]\n",
            "loss: 0.016323  [38400/50000]\n",
            "PSNR: 17.958383538643744\n",
            "Test Loss: 0.016019996397102935\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.015121  [    0/50000]\n",
            "loss: 0.015265  [12800/50000]\n",
            "loss: 0.015477  [25600/50000]\n",
            "loss: 0.014260  [38400/50000]\n",
            "PSNR: 17.997699883363683\n",
            "Test Loss: 0.015875936886649344\n",
            "Epoch 00048: reducing learning rate of group 0 to 2.0000e-08.\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.015877  [    0/50000]\n",
            "loss: 0.015224  [12800/50000]\n",
            "loss: 0.017458  [25600/50000]\n",
            "loss: 0.016540  [38400/50000]\n",
            "PSNR: 17.99511777312519\n",
            "Test Loss: 0.01588536827247354\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.015579  [    0/50000]\n",
            "loss: 0.015782  [12800/50000]\n",
            "loss: 0.015464  [25600/50000]\n",
            "loss: 0.015588  [38400/50000]\n",
            "PSNR: 17.995321347184994\n",
            "Test Loss: 0.015884631560950338\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.016591  [    0/50000]\n",
            "loss: 0.016791  [12800/50000]\n",
            "loss: 0.016063  [25600/50000]\n",
            "loss: 0.016432  [38400/50000]\n",
            "PSNR: 17.98841712478924\n",
            "Test Loss: 0.015909895265498495\n",
            "Epoch 00051: reducing learning rate of group 0 to 2.0000e-09.\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.016374  [    0/50000]\n",
            "loss: 0.015080  [12800/50000]\n",
            "loss: 0.015791  [25600/50000]\n",
            "loss: 0.017029  [38400/50000]\n",
            "PSNR: 17.98631990329113\n",
            "Test Loss: 0.015917507943378973\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.014767  [    0/50000]\n",
            "loss: 0.015455  [12800/50000]\n",
            "loss: 0.014744  [25600/50000]\n",
            "loss: 0.015947  [38400/50000]\n",
            "PSNR: 17.994871021519614\n",
            "Test Loss: 0.015886263898279095\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.016863  [    0/50000]\n",
            "loss: 0.016614  [12800/50000]\n",
            "loss: 0.015558  [25600/50000]\n",
            "loss: 0.017290  [38400/50000]\n",
            "PSNR: 17.994843503402436\n",
            "Test Loss: 0.015886363372961176\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.016507  [    0/50000]\n",
            "loss: 0.015916  [12800/50000]\n",
            "loss: 0.014678  [25600/50000]\n",
            "loss: 0.016688  [38400/50000]\n",
            "PSNR: 17.987134690651423\n",
            "Test Loss: 0.015914594000087507\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.015649  [    0/50000]\n",
            "loss: 0.014528  [12800/50000]\n",
            "loss: 0.015501  [25600/50000]\n",
            "loss: 0.015705  [38400/50000]\n",
            "PSNR: 17.99817035618258\n",
            "Test Loss: 0.01587421103981854\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.016374  [    0/50000]\n",
            "loss: 0.016683  [12800/50000]\n",
            "loss: 0.015454  [25600/50000]\n",
            "loss: 0.015928  [38400/50000]\n",
            "PSNR: 17.995603619146042\n",
            "Test Loss: 0.01588360790776301\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.018424  [    0/50000]\n",
            "loss: 0.015836  [12800/50000]\n",
            "loss: 0.015723  [25600/50000]\n",
            "loss: 0.015375  [38400/50000]\n",
            "PSNR: 17.99134262557237\n",
            "Test Loss: 0.01589915322587837\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.016032  [    0/50000]\n",
            "loss: 0.016448  [12800/50000]\n",
            "loss: 0.015739  [25600/50000]\n",
            "loss: 0.015681  [38400/50000]\n",
            "PSNR: 17.996082512509453\n",
            "Test Loss: 0.015881843546617636\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.015773  [    0/50000]\n",
            "loss: 0.015695  [12800/50000]\n",
            "loss: 0.015669  [25600/50000]\n",
            "loss: 0.015717  [38400/50000]\n",
            "PSNR: 17.996481258258374\n",
            "Test Loss: 0.015880389174422884\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model\n",
        "torch.save(model.state_dict(), \"model-data_augmented_tanh-coat0.pth\")\n",
        "print(\"Saved PyTorch Model State to model-data_augmented_tanh-coat0.pth\")"
      ],
      "metadata": {
        "id": "ZHRQMNasT0e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download( \"model-data_augmented_tanh-coat0.pth\" ) "
      ],
      "metadata": {
        "id": "QnDqNGb-rSTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MergedAutoEncoder()\n",
        "model.load_state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khNjvtG1xYbe",
        "outputId": "a777cf6b-53ae-4823-c244-4e78dd264972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The last LR is 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "mVomsZXoV6MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for test_images, test_labels in testloader:  \n",
        "    sample_image = test_images[0]    \n",
        "    sample_label = test_labels[0]"
      ],
      "metadata": {
        "id": "1-35kBV1Wie_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(sample_image.reshape(3,32,32).permute(1,2,0))"
      ],
      "metadata": {
        "id": "Pr5TZIwtXYUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fPFUu5-1tge0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): \n",
        "  prediction = model(sample_image.unsqueeze(0).to(device))\n",
        "  plt.imshow(prediction.cpu().reshape(3,32,32).permute(1,2,0))\n"
      ],
      "metadata": {
        "id": "RwpVoypmZWv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for train_images, _ in trainloader:  \n",
        "    sample_train_image = train_images[0] \n",
        "    i += 1\n",
        "    if i == 3:\n",
        "      break  \n",
        "plt.imshow(sample_train_image.reshape(3,32,32).permute(1,2,0))   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "jJ_6TQ0HjylG",
        "outputId": "b66916f8-6489-4281-e64f-8f0b300ea68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f31c057d610>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): \n",
        "  prediction = model(sample_train_image.unsqueeze(0).to(device))\n",
        "  plt.imshow(prediction.cpu().reshape(3,32,32).permute(1,2,0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "kriqPwKikrez",
        "outputId": "2b93db0a-7e73-4207-efcb-1b3e2e6ca227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAf3ElEQVR4nO2da4xkZ5nf/0/dq7uq790z7emxZ3xfc7OdkQFhEbJkkYOQwFGC4ANyJLJeZUEK0uaD5UiBSPnArgKIDxGRCdZ6N1yzgHCyaFnHQnFYrQxjML6buXjGMz19nb53VXXdnnyommTsvP/T7enp6oH3/5NGU/0+9Z7znvecp07V+z/P85i7Qwjxu09qvwcghOgNcnYhIkHOLkQkyNmFiAQ5uxCRIGcXIhIyu+lsZvcB+CqANID/4u5fTHr/QLnfJ0aHg7ZUNk/7ZXNFYuGyYdvb1JYkN/JegJH2rVqN9tlqNfk4EvbVbvORtBtbCf3C+0un+Od6Op2mthQ7aACZhG1mc9mwIWF7SeMwSxhjhl/GbXaunQ8knc7xfRk/Zk84o62E88mwNh+je/g8z87MYmVlNdjxip3dOrP/nwD8AYDzAH5hZo+7+0usz8ToMP7s858J2voO3kT3dfjQO4Lt7nXap5LgEFuNBrVV0aK2rIUn/7UTJ2ifU6sLfBxtfnHUNjepbWP+Nb7NjaVge6nIPjCBwaFBaisRnwWA0b5+ajtweDzYbhnutOXRAWrLZsvUNjAyQW2VRtgp2i3utOXB66mtL9NHbZ5w7VysrlNbuh12w0yNO3tjazHY/i//xR/TPrv5Gn8PgJPufto7XvcdAB/dxfaEEHvIbpz9EIBzl/19vtsmhLgG2fMFOjN70MyOm9nx1Q3+1VQIsbfsxtmnARy+7O+pbtsbcPdH3P2Yux8bLPHfeEKIvWU3zv4LALeY2VEzywH4BIDHr86whBBXmytejXf3ppl9FsBP0JHeHnX3F5P6rDa28Dczp4O2687M0n4vDsyEtzdDF/4xs8FXwVebfBV8cvQgtd15Y1gVOHdhnvZJpbkqUACXePr5wi5GwVeEc0TBnLiOL6eMjPBjzqW5dJjuK1FbJhdWSrzIl/dHsgVqK47w8Q+U+Wo8PDz/bXB1IlcYorZ2O0EeTNBScwlSXy4ddsOVPj5XlYvhnXmCRLkrnd3dfwzgx7vZhhCiN+gJOiEiQc4uRCTI2YWIBDm7EJEgZxciEna1Gv9WSTXaKJyvBm0r7bC8BgCrlQvB9o0FHlwwt7hBbZU217UO93EZam0hLONsZrhU07zIJcXRI/who7EUPzWFcR4UYiQ2KJVwzKk0l/Iq4MFGi3PhoBsA2CyG7yNvu4EHPFmRz32zyaMi28YDeVLNcETiWkLk4xhXG5HJcjkslSDLlfNcZnUiy42neMBWIx+WKZOiG3VnFyIS5OxCRIKcXYhIkLMLEQlydiEiober8dks+qauC9pqnpAaqR1efR5M89xvrcz/F237f6nW+ApziS+oot0Ir/6nG/wzc7rKY/gH53hesvrAGLWl+3jASL22FmzP1flqfC3FV9VbzlWNhQY/tlQ7vPpcLvPjyqS4ypDK8ECSakLKp3w6PFeZhO15Qg66TDYhPx34xVOr8mu1STIfblYqtM90dSXYXk9SXahFCPE7hZxdiEiQswsRCXJ2ISJBzi5EJMjZhYiEnkpvDQPmU+EAhGyOSxqtpYvB9tnlsPwAAAtLXHorpXkww2aV2wZGw7JhdZ2PY3NtjtpWCgkBKPWESjLGAyTaFZKbLMMDSa5PTVFbo8yDfNIJUlk6Hz62fF+4/BcApEu8IkyqzY+5r8SlyGojPB8bTS4blot8HLmEUlPNJpdSW+S6B4BmLSxvNrFK+zQ2w9e3t7msrDu7EJEgZxciEuTsQkSCnF2ISJCzCxEJcnYhImFX0puZnQGwDqAFoOnux5Len8qk0D8eLruzvsKlspdOPx/us8CliXxCKZ5GQuTSQoHbptJhaWWowCOo3n4TL02UNy7/bGzwiKe5WT5XtXb4lA4WeLmjC1kuy40Nc3ltqDxJbWuVcPTd9AIvlTWVMMb+Pj6OjRqfqw0P5zxcqnC5dDjP99WfkIOukRBxZgnSYY644UaDX1d9hfAYU3tV/qnLP3L3xauwHSHEHqKv8UJEwm6d3QH8rZk9Y2YPXo0BCSH2ht1+jb/X3afNbALAE2b2irs/dfkbuh8CDwJAaYRnoxFC7C27urO7+3T3/3kAPwRwT+A9j7j7MXc/VizzZ8GFEHvLFTu7mfWbWfnSawAfAvDC1RqYEOLqspuv8QcA/NDMLm3nW+7+N0kdCpkibh1/W9D2yrkTtF/Jwjpas86jjNotLoOsJpSNOrvKJZJDE+EItrEhnkTx4DiXp5CQsPEUXqe2TIaXyirWwiWqsnVe06hS4zLU0ky49BYAzOXC+wKAC2dPBtuXV/jYj9n7qW1siEffjY3y+fda+H42ljpA+6Qa3C22eFAZqusJCVDb/Fz3FcJlwPqLJdpnqBqe+/ReSG/ufhrAu660vxCit0h6EyIS5OxCRIKcXYhIkLMLEQlydiEioacJJ5vNOhYuhqWXepPLOBODYWkis8FlsvlFLq9hi4fEXawvU9upEz8PtrePhOVEAMi1wskyAWB27jS1bYLLOLmEWnXZZlh62djg27uwyMeR7uOyXKHAn4hcr4ajzdIpfn+5boqPY7J0F7WlnEeilVPhyLEaEmrf1bhMVnc+9+urPAqz6Pz6zlrYDQsJ9ehS+XCkopHjBXRnFyIa5OxCRIKcXYhIkLMLEQlydiEioaer8fVGHdPnwwEeZjz8dWIinBcuu8pXTaf6eXDEhXLCSuwCL7s00h9e6R4u8pXWrRrfV2uVKwYTo7yk0fWHeHDN6bPh1f+VjfDqOADwEBkg1ebKxcjE9dR2+813BNszBZ7vbjRhdT+b4gEe5Sxfga41wopNOcuPq5hQKiuf4Sv/hT5+DXuLu1qKnIFWm+dD7CfDT6gypTu7ELEgZxciEuTsQkSCnF2ISJCzCxEJcnYhIqGn0lur3cJqNRxocusoz7dVnw6XO/Ial5Nuu4XLQu/9PS7xzJzmpX+aJDilscEDINYqXHrbbPHP2nKFyy7zGzxYZ2ElHLiSInn8AGA8x2W+AwcOUltphOdxG8gNB9trbX7OVpb5PC7M8Vx4iXJTKjz/YyPjtM/wwCjfXoafl3wfv64adS6zVjbDtrbzHIvVevi4EpRS3dmFiAU5uxCRIGcXIhLk7EJEgpxdiEiQswsRCdtKb2b2KICPAJh397d320YAfBfAEQBnAHzc3bkedGlbADIWjlC6uLZB+7XWw3m/Liakmetr8wikm24IR2QBQG2Bj+OF35wLtjezi7RPpc4jstDk098HrqGcu8Dzwi1shiPwRga4tDkxMkFtbXK+AGBtI1wOCwCWZ8PRjQtbPBeeG5cpCwUuDx48mFB+qxS+n93z3g/QPsMDvNRUIZ8gvWV4ZF4uzcefRjgyb61aoX3qrfA8Orhct5M7+58DuO9NbQ8BeNLdbwHwZPdvIcQ1zLbO3q23vvSm5o8CeKz7+jEAH7vK4xJCXGWu9Df7AXe/lBN6Fp2KrkKIa5hdL9C5uwP8B6aZPWhmx83s+FaF/14TQuwtV+rsc2Y2CQDd/+fZG939EXc/5u7H8n18kUIIsbdcqbM/DuCB7usHAPzo6gxHCLFX7ER6+zaADwAYM7PzAD4P4IsAvmdmnwZwFsDHd7KzRqOB6elw9FJ2JCGRn4cT8q2scO3txKunqC1X5dFVZ8+GI+wA4OSF8P4GEiSXVEKCwoPjA9SWqfGfPJVNLsnUGmHpZTkhOWelGS7JBQDFUrj0FgBkq3yMLZJgsZpwXIsr/LzUq3z8031Fausrh5NHbm7x0mFTIzxicurGm6gtl5BM09L8Omi1w3PVXkmQo9vh+7Q7l0q3dXZ3/yQxfXC7vkKIawc9QSdEJMjZhYgEObsQkSBnFyIS5OxCREJPE062W01srYaD49plHnl1+3A4AWBleJb2mb3AI9HSdV6bbXWNS0NpC0/XcJZH2B0+ciu1ZYh8AgAN8DE2C3yM1c218PZaPKKsnPCRX2/yfQ1meHLOQjb8AFUpxXe2tsWj+WyLRwF6NiE55/mwnPeTnzxF+wzneNTbP/3nn6K2ycOHqS2V5pKYZcK2VJ4/hFYohOXoVML86s4uRCTI2YWIBDm7EJEgZxciEuTsQkSCnF2ISOip9JZJZTBaCtcAmyzz6Kq0hyWe8TyPdlrN8qim80u83lgmxSPY2q2wRGIJNb42a3wc9Ra3LTQSbAkRbE0Lf373lblMVszwZJQbdT5X52dpGgMMl8NJQocS6sqN9PPziQR5rTjGE06emw+fz6WVN2da+3/88Mm/o7ajN91JbeVxLh/3J0TmOcJjTGd5FF2xLxzdKOlNCCFnFyIW5OxCRIKcXYhIkLMLEQk9XY03yyCXDa+crl/kedVmquGyS6VyOEAGAGbmEnK41XiQSTrDAy7qrbDtwuJF3qfBV7NrWb7yv1QNBzoASEjcDeTIqvWBobAKAgADJR7AYQs82Mi3+Ir2SCm86j5aSFixbvJgkfUcP+ihAX5sy2FRANNzvHTV6xfCeRIB4Ps/+xW1Hb373dR2aIK7WqsVHmQjxftkSE47A59D3dmFiAQ5uxCRIGcXIhLk7EJEgpxdiEiQswsRCTsp//QogI8AmHf3t3fbvgDgDwEsdN/2sLv/eLttZXNZjB8KV3duVbl8VdsIS1SW5eWT1sFz0K2Ay2ETaR58UG2Fy/GEQxIuGROkEOMlgQ5PJUhUZZ5PbrUSlnHceLDO6OgUtZUGeL+NBi/XNJULB9705XjQzZktopMBqKwuUFt+k5+BZjN87VTqPNComVBq6te/eonanvzFSWr7B++8kdr6+8Ky4liBB//kSZCM2e6ktz8HcF+g/Svufmf337aOLoTYX7Z1dnd/CgB/ekII8VvBbn6zf9bMnjOzR82MP8IkhLgmuFJn/xqAmwDcCWAGwJfYG83sQTM7bmbHqwmlhoUQe8sVObu7z7l7y93bAL4O4J6E9z7i7sfc/VixnxdTEELsLVfk7GY2edmf9wN44eoMRwixV+xEevs2gA8AGDOz8wA+D+ADZnYnOvFXZwD80U525uZo5cPRXPktLp9kS2GZodXHvynUEj7HJsdvoLbhApeGykNhaaiV4hJaPs1lsnRCzrWpiYN8HOP82F4/czbYnitzCS2TkOssO8T72TyXeZAOn5vVhJx8ixUuea3XeBRjO8VludoWkVlbfHuekBtwfS0cgQkAzz//99SGHN/mSH84CvOWm2/nfTzsR+02v962dXZ3/2Sg+Rvb9RNCXFvoCTohIkHOLkQkyNmFiAQ5uxCRIGcXIhJ6m3Cy7UhvhOWrTEJ0WGkwLIdVG+u0z9jkOLVNjV9HbcM5LqM1B4aC7dkCL61U3VimtgtzZ6htrsH78TSVQLqPREPxw8KFJp/Hvi0e2VYa4aWc4OHzbE0usWaSnrmq8uujCH5w44PhsmKrY3x+q87lq+E8P+bUJo/aW53mUZjV/nDU2+gwjwStNsPSW4O0A7qzCxENcnYhIkHOLkQkyNmFiAQ5uxCRIGcXIhJ6Kr2lUikM9IeTRBYS5JMy0Zoq8zxb1vhwWCYDgEyBR3nNLfEIqsFiOILqwM1c5jt7gkc7La/zyKt6jctho0ReAwDPhOWftfUV2mdzbYbapsbCtfkAIF+apLZCNixHrmxO0z7e5DX4xkf5HB8YOERtTIqcuOE22mdhkUe2NVb4eUk1wglJAcCba9Q2sxCWNyu/4dfOWDrsutWEpJ26swsRCXJ2ISJBzi5EJMjZhYgEObsQkdDT1fhMOovRoXBZo5zzlem1uVPB9tenz9A+LfBccrbGV9wXZmeprbAcDqrYbPLPzKUlvgq+WuErtAnxFqjneDBJayu8Iry0xvO7tY3nwuOhGEApE54PAJheCgeazFw8TfvkW+GAEAAYnzhMbe782Jp94Uu8v8hLh22tcbc4tc4VoPrM63ybDT7Gxa1wkEz1Fd7n4IGwErJZTVALqEUI8TuFnF2ISJCzCxEJcnYhIkHOLkQkyNmFiISdlH86DOAvABxAp9zTI+7+VTMbAfBdAEfQKQH1cXfnib0ApFOGASKFoMrzfp2dnQ+2X1zigQf9pMwUANSaOWpbq/NgDFTDY08Z3146QZYbKPGkawP9/NRMFrlElWmFx7JR5cdVbfEgpJvHDlDbxAgPTnl1PRxM0q7wSr6loRFqKza43HihwmXboVRYVhxoha8pAFhc4oFBFze5XNre4AFKNsKPDSR33XKCzJez8LXTbHA/2smdvQngT9z9DgDvAfAZM7sDwEMAnnT3WwA82f1bCHGNsq2zu/uMu/+y+3odwMsADgH4KIDHum97DMDH9mqQQojd85Z+s5vZEQB3AXgawAF3v/R9Zxadr/lCiGuUHTu7mZUAfB/A59z9DT9c3N3R+T0f6vegmR03s+Mb6/w3thBib9mRs5tZFh1H/6a7/6DbPGdmk137JIDgioe7P+Lux9z9WKnMn1cXQuwt2zq7mRk69dhfdvcvX2Z6HMAD3dcPAPjR1R+eEOJqsZOot/cB+BSA583s2W7bwwC+COB7ZvZpAGcBfHzbnaUzGBscDdo8zXNntUhJm7Uql3EODfJDa2R5lNdQMTw+AGimhoPt9Q0ud2xscZms0eC2dEIZqlxCrjb3cJmkVoXnMyuN8ei1Q5NHqa08OEht9bOvhPdV4jLlgQI/Zw4upWbArwMrhqPlVqbP0j7rm3x7GwnXXGqVl8o6ee4EtVUq4dyGi0s85rBC5Lr6Fr82tnV2d/8ZAFZo64Pb9RdCXBvoCTohIkHOLkQkyNmFiAQ5uxCRIGcXIhJ6mnASaKPVCktAXHgDLizMBdtL/UwkAKau4w/wTB39PWqbOc/ljmdfOR9sf/UEj5Kq1rjkhQL/rG3lw4k5AaAxklDaqh3eX9tIDS0AjU3+ZOP0EpeMSqt8m5VKOOptqo9Lb+++lZdxen2Zj7GaIMtl0xfChhzvk8rwKLpGnctri7M8Sq1BkpUCwHozLN1uJchotWp4X826yj8JET1ydiEiQc4uRCTI2YWIBDm7EJEgZxciEnoqvTWahjki17xyJiyvAcDcZjjZYCrLZZxmvkxtM4vh2loA8OLLJ6nt9XNh2aUTBRymVOI1xQZbXOIpTPKIslSKR9lZK5z0cKzNx9Fyniixvc5lxbNNLr3l1sORXCPXH+TjqPMxvnYuLOUBwGyFy2FvK4UjFVNTt9A+A8vhsQPA1gavVdds8vOZJA86m8YMv64Kg+HjshRPiKk7uxCRIGcXIhLk7EJEgpxdiEiQswsRCT1djW/Bsc5CXnJ8ZT0zOhVsL2TWaZ/zK3z1c2vtDLW9eo6vPi+vhcfeb3waiyk+jmaGr2ZvzC5QW73BP6ObpXCZoWKZqxOpJs93t7g4S22W46vFbIl5dpEHi6ytcSXkuZM8IGelyYONRofCQUN3Xxe+pgBgqJ+fM7QTzmfCONJpnm8wnw+rIUfefRftc8vt9wTbn/zLx4LtgO7sQkSDnF2ISJCzCxEJcnYhIkHOLkQkyNmFiIRtpTczOwzgL9ApyewAHnH3r5rZFwD8IYBLGtHD7v7jpG2V+vtw77GwnLA+wwMkJkfDtpOn/p72ybd5QEB9o0BtzTYvDVVZD+cEq7R53q9GlUs1a0UegLLe4tLb7OomtVWz08H2TEIgRgpcAkSD50EbKfO5snR4f9c7v7/05/i+EioyIZvl0mG1Eh5jczkcSAIAm8vhwCsAaLe5LZXi89hf5Dno7j323mD7H3/uYdpn4uB4sP2lH/817bMTnb0J4E/c/ZdmVgbwjJk90bV9xd3/4w62IYTYZ3ZS620GwEz39bqZvQyApwEVQlyTvKXf7GZ2BMBdAJ7uNn3WzJ4zs0fNjH8vEkLsOzt2djMrAfg+gM+5+xqArwG4CcCd6Nz5v0T6PWhmx83s+Ory8lUYshDiStiRs5tZFh1H/6a7/wAA3H3O3Vvu3gbwdQDBh3Xd/RF3P+buxwaHdfMXYr/Y1tmtk3PpGwBedvcvX9Y+ednb7gfwwtUfnhDiarGT1fj3AfgUgOfN7Nlu28MAPmlmd6Ijx50B8EfbbaiQzeG2yRuCtvzBw7TfHbffHmw/dfoI7VNf53nmijkug/zXb32L2v77/E+D7Q1ScgkAqm0uxxwa4iWqBif6qG3JuQ61uhb+qVSrc8kILR69VhzmY6wnKHYTw+Fos0ND/Dyn8zzv3sQGzwu3uMV/HnoxLLP+73Mv0j4vvM4j7Nrg0Wu5HJcAbz96HbXdf/+Hgu1HjoTlNQDIengc6YTb905W438GIHQ1JGrqQohrCz1BJ0QkyNmFiAQ5uxCRIGcXIhLk7EJEQk8TTqZTKQwWw1FIaeOfO8VCWD7pu43LU40EqSnNqyfh7Tdy2eWvC+Eou2KCBHXdFJdPPnj3u6jttnvCkVAAMD9/htrObIaj5S7O8PJa6SaXIieOTlLb1BAPkTiQCctQi4s8+u7c2TPUlh/mstxh42Wj1trhRKbLF16hfVYrvIwTkqS3Ik+a2nbuan/3bFgGLAzwh9CO3hyWo5stfnHrzi5EJMjZhYgEObsQkSBnFyIS5OxCRIKcXYhI6Kn05u02toiskcnwobQa4T4ryxdpn5X1FWrLJ9Tdmm+GEzYCQL4Yjg5rp7jkUiBSIwCkSGQYANz+Ni7L/cN//BFqy2Ij2N5scClyYZ7PVXGAS17FIpc+vRKOzDt78je0T5YrgLgOPOrNcjyBKErh8S+8xiXRjf/FoxjPv3aW2jIJGuyS8/H/6sQzwfb8LTxCMDUUrulXT6g3pzu7EJEgZxciEuTsQkSCnF2ISJCzCxEJcnYhIqGn0lu71cbmWlgayqS5fJUiefxqLS6hbVXC+wGApRqPalpNSG3vpA5co8Y7rTd4lFelxWWS/hKXhsr9YdkFADLZcL9qwnyU+3mtOk/IU5lPc62s2Re+tIYm+HEdzfHEl5ODvBZgujRGbSiH5c3ld3G5cWaEy41PfPOvqK1e5zX41jcSitUVw3O1tHCedmm/493Bdk+IHtWdXYhIkLMLEQlydiEiQc4uRCTI2YWIhG1X482sAOApAPnu+//K3T9vZkcBfAfAKIBnAHzK3fmyLgAzIJsPBwvkSZmeDuG8WocP8FXY8YQgjY1KOE8bANx621Fq6386/Nlo4GV/tsCnZGUzQRVY5YETpSK3tdpbwfbpxdf5OM6forahEZ6DLpdQrqnZCJ+z9U2eI20gU6a2Qj/fV3qQXwdNcjvLDfLtve8dd1Db+Ttvo7aZ2deorZ6wGj92w4Fg++13vIP2GR8M593LJtR/2smdfQvA77v7u9Apz3yfmb0HwJ8C+Iq73wxgGcCnd7AtIcQ+sa2ze4dLIm22+88B/D6AS6LjYwA+ticjFEJcFXZanz3dreA6D+AJAKcArLj7pSdGzgPgeYWFEPvOjpzd3VvufieAKQD3AAgnrQ5gZg+a2XEzO35xaekKhymE2C1vaTXe3VcA/BTAewEMmdmlBb4pAMEUL+7+iLsfc/djoyP8MU8hxN6yrbOb2biZDXVfFwH8AYCX0XH6f9Z92wMAfrRXgxRC7J6dBMJMAnjMzNLofDh8z93/h5m9BOA7ZvYfAPwKwDd2ssN0KxwY0qrzgJF8OizXpRMCMdL9eWqr1bj801/k2xyaGA22txLktcEBLietG5fsfvnSc9T26iKXeFYvzobbNxLyzDVL1HZTdoLacstcOkylwkFKVecBT9jkgSS5Cp/jTG6N2lgVsHSGB90cLvNgnfvueT+1zS/eQG2VhLJMwwNh6fAD17+T9hkEyeWYUJ5qW2d39+cA3BVoP43O73chxG8BeoJOiEiQswsRCXJ2ISJBzi5EJMjZhYgEc+dL9Vd9Z2YLAC7VzxkDsNiznXM0jjeicbyR37Zx3ODuQe2wp87+hh2bHXf3Y/uyc41D44hwHPoaL0QkyNmFiIT9dPZH9nHfl6NxvBGN4438zoxj336zCyF6i77GCxEJ++LsZnafmb1qZifN7KH9GEN3HGfM7Hkze9bMjvdwv4+a2byZvXBZ24iZPWFmJ7r/D+/TOL5gZtPdOXnWzD7cg3EcNrOfmtlLZvaimf3rbntP5yRhHD2dEzMrmNnPzezX3XH8+277UTN7uus33zWzhBDCAO7e038A0uiktboRQA7ArwHc0etxdMdyBsDYPuz3/QDuBvDCZW1/BuCh7uuHAPzpPo3jCwD+TY/nYxLA3d3XZQC/AXBHr+ckYRw9nRMABqDUfZ0F8DSA9wD4HoBPdNv/M4B/9Va2ux939nsAnHT3095JPf0dAB/dh3HsG+7+FIA35+j6KDqJO4EeJfAk4+g57j7j7r/svl5HJznKIfR4ThLG0VO8w1VP8rofzn4IwLnL/t7PZJUO4G/N7Bkze3CfxnCJA+4+0309CyCcTLw3fNbMnut+zd/znxOXY2ZH0Mmf8DT2cU7eNA6gx3OyF0leY1+gu9fd7wbwTwB8xsx4GpIe4p3vafslk3wNwE3o1AiYAfClXu3YzEoAvg/gc+7+hvQzvZyTwDh6Pie+iySvjP1w9mkAhy/7myar3Gvcfbr7/zyAH2J/M+/MmdkkAHT/n9+PQbj7XPdCawP4Ono0J2aWRcfBvunuP+g293xOQuPYrznp7vstJ3ll7Iez/wLALd2VxRyATwB4vNeDMLN+Mytfeg3gQwBeSO61pzyOTuJOYB8TeF5yri73owdzYmaGTg7Dl939y5eZejonbBy9npM9S/LaqxXGN602fhidlc5TAP7tPo3hRnSUgF8DeLGX4wDwbXS+DjbQ+e31aXRq5j0J4ASA/wlgZJ/G8ZcAngfwHDrONtmDcdyLzlf05wA82/334V7PScI4ejonAN6JThLX59D5YPl3l12zPwdwEsB/A5B/K9vVE3RCRELsC3RCRIOcXYhIkLMLEQlydiEiQc4uRCTI2YWIBDm7EJEgZxciEv4P8gljiekxap8AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Pred: {prediction}, Sample: {sample_train_image}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbhoY4b9jBeL",
        "outputId": "3b9ee6fb-543d-4097-d24d-657c74415693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pred: tensor([[[[-0.0304,  0.5053,  0.5022,  ...,  0.4445, -0.0041,  0.5285],\n",
            "          [ 0.6265,  0.6073,  0.4726,  ..., -0.0562,  0.4721,  0.4507],\n",
            "          [ 0.6039,  0.5291,  0.3885,  ...,  0.4588, -0.0157,  0.5798],\n",
            "          ...,\n",
            "          [ 0.1844,  0.1843, -0.0288,  ...,  0.2814,  0.2144,  0.2324],\n",
            "          [ 0.2037,  0.1733,  0.2273,  ...,  0.2141,  0.1032,  0.2038],\n",
            "          [ 0.3970,  0.2690,  0.2171,  ..., -0.1121,  0.3130,  0.3311]],\n",
            "\n",
            "         [[ 0.6885,  0.7504,  0.8962,  ...,  0.5988,  0.6479,  0.7084],\n",
            "          [-0.0025,  0.8644,  0.9975,  ...,  0.7763,  0.7665,  0.7744],\n",
            "          [ 0.7807,  0.8433,  0.9086,  ...,  0.8546,  0.7004,  0.6793],\n",
            "          ...,\n",
            "          [ 0.3536,  0.3896,  0.3778,  ...,  0.3353,  0.2713,  0.3668],\n",
            "          [ 0.4496,  0.3269,  0.3944,  ...,  0.3700, -0.0211, -0.0090],\n",
            "          [-0.0442, -0.0017,  0.4832,  ..., -0.0548, -0.1353,  0.2882]],\n",
            "\n",
            "         [[ 0.9462,  1.0128, -0.0169,  ...,  0.8885,  0.8781,  0.8226],\n",
            "          [-0.0101,  1.0426,  0.8199,  ..., -0.0186, -0.0026,  0.8233],\n",
            "          [-0.0030,  1.0714, -0.0023,  ..., -0.0237,  0.6976,  0.9769],\n",
            "          ...,\n",
            "          [-0.0179,  0.4379,  0.4446,  ...,  0.5007,  0.4531,  0.4696],\n",
            "          [ 0.5329, -0.0070, -0.0478,  ...,  0.4503, -0.0658,  0.4075],\n",
            "          [ 0.4900,  0.4509,  0.5287,  ...,  0.5074,  0.4829,  0.4133]]]],\n",
            "       device='cuda:0'), Sample: tensor([[[0.5922, 0.5882, 0.5961,  ..., 0.4196, 0.4353, 0.4431],\n",
            "         [0.5882, 0.5843, 0.5922,  ..., 0.4275, 0.4392, 0.4471],\n",
            "         [0.5961, 0.5922, 0.5922,  ..., 0.4196, 0.4353, 0.4431],\n",
            "         ...,\n",
            "         [0.1804, 0.2275, 0.2196,  ..., 0.1412, 0.1059, 0.0941],\n",
            "         [0.2000, 0.2039, 0.1922,  ..., 0.1529, 0.1451, 0.1294],\n",
            "         [0.1843, 0.1059, 0.1333,  ..., 0.0902, 0.1059, 0.1216]],\n",
            "\n",
            "        [[0.8549, 0.8431, 0.8549,  ..., 0.7529, 0.7647, 0.7647],\n",
            "         [0.8471, 0.8392, 0.8471,  ..., 0.7608, 0.7686, 0.7647],\n",
            "         [0.8549, 0.8471, 0.8471,  ..., 0.7569, 0.7686, 0.7647],\n",
            "         ...,\n",
            "         [0.2706, 0.3176, 0.3333,  ..., 0.2392, 0.2118, 0.2078],\n",
            "         [0.2784, 0.2745, 0.3020,  ..., 0.2431, 0.2392, 0.2314],\n",
            "         [0.2392, 0.1647, 0.2235,  ..., 0.1804, 0.1882, 0.1961]],\n",
            "\n",
            "        [[1.0000, 0.9922, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "         [0.9961, 0.9843, 0.9961,  ..., 1.0000, 1.0000, 0.9961],\n",
            "         [1.0000, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 0.9882],\n",
            "         ...,\n",
            "         [0.3922, 0.4510, 0.4667,  ..., 0.3451, 0.3373, 0.3294],\n",
            "         [0.3725, 0.3765, 0.4000,  ..., 0.3451, 0.3569, 0.3412],\n",
            "         [0.3137, 0.2353, 0.3059,  ..., 0.2667, 0.2824, 0.2980]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_train_image[:,1,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otSnlnecZotm",
        "outputId": "d4a1a824-3dab-4815-b28f-4e9edc9c902e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5882, 0.8471, 0.9961])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction.cpu().reshape(3,32,32).permute(1,2,0)[1,0,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsOjExYqYf1U",
        "outputId": "076199e6-410f-47fa-9566-f32254504da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.6265, -0.0025, -0.0101])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_img = torch.rand((1,3,32,32))\n",
        "\n",
        "upsampled_test = nn.functional.interpolate(test_img,scale_factor = 2, mode = 'bicubic')\n",
        "print(upsampled_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-vxnaUqAhA-",
        "outputId": "d57f4ad7-9dfa-4d14-aa90-2202b13ff949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_img = torch.ones((5,4,3))*0.3\n",
        "test_img[[0,1],:,:] += torch.ones(2,4,3)*0.5\n",
        "test_img[:,:,[0 ,1]] *= -1\n",
        "plt.imshow(test_img)\n",
        "print(test_img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "-Vabv740gDE2",
        "outputId": "669adc38-1a23-4ca0-d49a-04f7f04df73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.8000, -0.8000,  0.8000],\n",
            "         [-0.8000, -0.8000,  0.8000],\n",
            "         [-0.8000, -0.8000,  0.8000],\n",
            "         [-0.8000, -0.8000,  0.8000]],\n",
            "\n",
            "        [[-0.8000, -0.8000,  0.8000],\n",
            "         [-0.8000, -0.8000,  0.8000],\n",
            "         [-0.8000, -0.8000,  0.8000],\n",
            "         [-0.8000, -0.8000,  0.8000]],\n",
            "\n",
            "        [[-0.3000, -0.3000,  0.3000],\n",
            "         [-0.3000, -0.3000,  0.3000],\n",
            "         [-0.3000, -0.3000,  0.3000],\n",
            "         [-0.3000, -0.3000,  0.3000]],\n",
            "\n",
            "        [[-0.3000, -0.3000,  0.3000],\n",
            "         [-0.3000, -0.3000,  0.3000],\n",
            "         [-0.3000, -0.3000,  0.3000],\n",
            "         [-0.3000, -0.3000,  0.3000]],\n",
            "\n",
            "        [[-0.3000, -0.3000,  0.3000],\n",
            "         [-0.3000, -0.3000,  0.3000],\n",
            "         [-0.3000, -0.3000,  0.3000],\n",
            "         [-0.3000, -0.3000,  0.3000]]])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transpose Convolution Test"
      ],
      "metadata": {
        "id": "JdVb6oxPu0EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_img = torch.rand((1,3,4,4))\n",
        "conv1 = nn.Conv2d(3,6,9,stride=2,padding = 4)\n",
        "tconv1 = nn.ConvTranspose2d(6,3,9,stride=2, padding = 4, padding_mode = 'zeros', output_padding =1)"
      ],
      "metadata": {
        "id": "mSu425L3uz3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = conv1(test_img)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ0xxACzvc4m",
        "outputId": "de7effd9-8a97-45a8-d728-b294039347fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = tconv1(out)\n",
        "print(inp,test_img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhtpaPaCvk1v",
        "outputId": "6cc282c2-d00e-4346-f049-eb6b7b3ae4bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-0.0086, -0.0083,  0.0496, -0.0033],\n",
            "          [ 0.0780, -0.0133,  0.0247, -0.0376],\n",
            "          [ 0.0161, -0.0291, -0.0205,  0.0035],\n",
            "          [ 0.0167, -0.0012,  0.0644,  0.0330]],\n",
            "\n",
            "         [[-0.0610, -0.0690, -0.1254, -0.0404],\n",
            "          [-0.0793, -0.0464, -0.0442, -0.0613],\n",
            "          [-0.0379, -0.0891, -0.0516, -0.0814],\n",
            "          [-0.0920, -0.1054, -0.0521, -0.1036]],\n",
            "\n",
            "         [[-0.0197, -0.0046, -0.0123, -0.0380],\n",
            "          [-0.0099, -0.0208, -0.0200,  0.0110],\n",
            "          [-0.0848, -0.0005, -0.0024, -0.0717],\n",
            "          [ 0.0500, -0.0403,  0.0310,  0.0222]]]],\n",
            "       grad_fn=<ConvolutionBackward0>) tensor([[[[0.6772, 0.3688, 0.6604, 0.7083],\n",
            "          [0.1725, 0.2043, 0.4134, 0.3527],\n",
            "          [0.9243, 0.7630, 0.9189, 0.0444],\n",
            "          [0.1588, 0.3466, 0.1642, 0.8677]],\n",
            "\n",
            "         [[0.7165, 0.6763, 0.6757, 0.2020],\n",
            "          [0.5091, 0.8607, 0.7486, 0.7892],\n",
            "          [0.2434, 0.6277, 0.0537, 0.3956],\n",
            "          [0.0805, 0.4254, 0.3593, 0.8802]],\n",
            "\n",
            "         [[0.6234, 0.9429, 0.7410, 0.5261],\n",
            "          [0.4805, 0.4569, 0.0560, 0.7741],\n",
            "          [0.2432, 0.9682, 0.5754, 0.5888],\n",
            "          [0.1571, 0.3224, 0.2683, 0.0341]]]])\n"
          ]
        }
      ]
    }
  ]
}