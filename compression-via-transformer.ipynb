{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install einops","metadata":{"_uuid":"54668519-229d-4f6c-b3c0-2e19e35403f0","_cell_guid":"ca0ba41c-6979-47c3-8fca-51eb88dcd066","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-07-29T09:50:35.321256Z","iopub.execute_input":"2022-07-29T09:50:35.321878Z","iopub.status.idle":"2022-07-29T09:50:48.043892Z","shell.execute_reply.started":"2022-07-29T09:50:35.321689Z","shell.execute_reply":"2022-07-29T09:50:48.042757Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\n\ndef lower_bound_fwd(x: Tensor, bound: Tensor) -> Tensor:\n    return torch.max(x, bound)\n\n\ndef lower_bound_bwd(x: Tensor, bound: Tensor, grad_output: Tensor):\n    pass_through_if = (x >= bound) | (grad_output < 0)\n    return pass_through_if * grad_output, None\n#Compute PSNR\nimport math\ndef compute_psnr(img1, img2):\n    img1 = img1.astype(np.float64) \n    img2 = img2.astype(np.float64) \n    mse = np.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return \"Same Image\"\n    return 10 * math.log10(1. / mse)\n\nclass LowerBoundFunction(torch.autograd.Function):\n    \"\"\"Autograd function for the `LowerBound` operator.\"\"\"\n\n    @staticmethod\n    def forward(ctx, x, bound):\n        ctx.save_for_backward(x, bound)\n        return lower_bound_fwd(x, bound)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, bound = ctx.saved_tensors\n        return lower_bound_bwd(x, bound, grad_output)\nclass LowerBound(nn.Module):\n    \"\"\"Lower bound operator, computes `torch.max(x, bound)` with a custom\n    gradient.\n\n    The derivative is replaced by the identity function when `x` is moved\n    towards the `bound`, otherwise the gradient is kept to zero.\n    \"\"\"\n\n    bound: Tensor\n\n    def __init__(self, bound: float):\n        super().__init__()\n        self.register_buffer(\"bound\", torch.Tensor([float(bound)]))\n\n    @torch.jit.unused\n    def lower_bound(self, x):\n        return LowerBoundFunction.apply(x, self.bound)\n\n    def forward(self, x):\n        if torch.jit.is_scripting():\n            return torch.max(x, self.bound)\n        return self.lower_bound(x)\n\n\nclass NonNegativeParametrizer(nn.Module):\n    \"\"\"\n    Non negative reparametrization.\n\n    Used for stability during training.\n    \"\"\"\n\n    pedestal: Tensor\n\n    def __init__(self, minimum: float = 0, reparam_offset: float = 2 ** -18):\n        super().__init__()\n\n        self.minimum = float(minimum)\n        self.reparam_offset = float(reparam_offset)\n\n        pedestal = self.reparam_offset ** 2\n        self.register_buffer(\"pedestal\", torch.Tensor([pedestal]))\n        bound = (self.minimum + self.reparam_offset ** 2) ** 0.5\n        self.lower_bound = LowerBound(bound)\n\n    def init(self, x: Tensor) -> Tensor:\n        return torch.sqrt(torch.max(x + self.pedestal, self.pedestal))\n\n    def forward(self, x: Tensor) -> Tensor:\n        out = self.lower_bound(x)\n        out = out ** 2 - self.pedestal\n        return out\nclass GDN(nn.Module):\n    r\"\"\"Generalized Divisive Normalization layer.\n\n    Introduced in `\"Density Modeling of Images Using a Generalized Normalization\n    Transformation\" <https://arxiv.org/abs/1511.06281>`_,\n    by Balle Johannes, Valero Laparra, and Eero P. Simoncelli, (2016).\n\n    .. math::\n\n       y[i] = \\frac{x[i]}{\\sqrt{\\beta[i] + \\sum_j(\\gamma[j, i] * x[j]^2)}}\n\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        inverse: bool = False,\n        beta_min: float = 1e-6,\n        gamma_init: float = 0.1,\n    ):\n        super().__init__()\n\n        beta_min = float(beta_min)\n        gamma_init = float(gamma_init)\n        self.inverse = bool(inverse)\n\n        self.beta_reparam = NonNegativeParametrizer(minimum=beta_min)\n        beta = torch.ones(in_channels)\n        beta = self.beta_reparam.init(beta)\n        self.beta = nn.Parameter(beta)\n\n        self.gamma_reparam = NonNegativeParametrizer()\n        gamma = gamma_init * torch.eye(in_channels)\n        gamma = self.gamma_reparam.init(gamma)\n        self.gamma = nn.Parameter(gamma)\n\n    def forward(self, x: Tensor) -> Tensor:\n        _, C, _, _ = x.size()\n\n        beta = self.beta_reparam(self.beta)\n        gamma = self.gamma_reparam(self.gamma)\n        gamma = gamma.reshape(C, C, 1, 1)\n        norm = F.conv2d(x ** 2, gamma, beta)\n        \n        if self.inverse:\n            norm = torch.sqrt(norm)\n        else:\n            norm = torch.rsqrt(norm)\n\n        out = x * norm\n\n        return out","metadata":{"_uuid":"6725db26-c8b5-4aeb-9014-f21b9df57918","_cell_guid":"d5b8e4a4-dca0-48c8-8622-a51cdaba4168","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-07-29T09:50:48.046421Z","iopub.execute_input":"2022-07-29T09:50:48.046810Z","iopub.status.idle":"2022-07-29T09:50:50.011296Z","shell.execute_reply.started":"2022-07-29T09:50:48.046771Z","shell.execute_reply":"2022-07-29T09:50:50.010337Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nfrom einops import rearrange\nfrom einops.layers.torch import Rearrange\n\n\ndef conv_3x3_bn(inp, oup, image_size, downsample=False):\n    stride = 1 if downsample == False else 2\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.GELU()\n    )\n\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn, norm):\n        super().__init__()\n        self.norm = norm(dim)\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\n\nclass SE(nn.Module):\n    def __init__(self, inp, oup, expansion=0.25):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(oup, int(inp * expansion), bias=False),\n            nn.GELU(),\n            nn.Linear(int(inp * expansion), oup, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass MBConv(nn.Module):\n    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n        super().__init__()\n        self.downsample = downsample\n        stride = 1 if self.downsample == False else 2\n        hidden_dim = int(inp * expansion)\n\n        if self.downsample:\n            self.pool = nn.MaxPool2d(3, 2, 1)\n            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n\n        if expansion == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n                          1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.GELU(),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                # down-sample in the first conv\n                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.GELU(),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n                          groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.GELU(),\n                SE(inp, hidden_dim),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        \n        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n    def forward(self, x):\n        if self.downsample:\n            return self.proj(self.pool(x)) + self.conv(x)\n        else:\n            return x + self.conv(x)\n\n\nclass Attention(nn.Module):\n    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n        super().__init__()\n        inner_dim = dim_head * heads\n        project_out = not (heads == 1 and dim_head == inp)\n\n        self.ih, self.iw = image_size\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        # parameter table of relative position bias\n        self.relative_bias_table = nn.Parameter(\n            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n\n        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n        coords = torch.flatten(torch.stack(coords), 1)\n        relative_coords = coords[:, :, None] - coords[:, None, :]\n\n        relative_coords[0] += self.ih - 1\n        relative_coords[1] += self.iw - 1\n        relative_coords[0] *= 2 * self.iw - 1\n        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n        self.register_buffer(\"relative_index\", relative_index)\n\n        self.attend = nn.Softmax(dim=-1)\n        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, oup),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        qkv = self.to_qkv(x).chunk(3, dim=-1)\n        q, k, v = map(lambda t: rearrange(\n            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        # Use \"gather\" for more efficiency on GPUs\n        relative_bias = self.relative_bias_table.gather(\n            0, self.relative_index.repeat(1, self.heads))\n        relative_bias = rearrange(\n            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n        dots = dots + relative_bias\n\n        attn = self.attend(dots)\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        out = self.to_out(out)\n        return out\n\n\nclass Transformer(nn.Module):\n    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n        super().__init__()\n        hidden_dim = int(inp * 4)\n\n        self.ih, self.iw = image_size\n        self.downsample = downsample\n\n        if self.downsample:\n            self.pool1 = nn.MaxPool2d(3, 2, 1)\n            self.pool2 = nn.MaxPool2d(3, 2, 1)\n            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n\n        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n        self.ff = FeedForward(oup, hidden_dim, dropout)\n\n        self.attn = nn.Sequential(\n            Rearrange('b c ih iw -> b (ih iw) c'),\n            PreNorm(inp, self.attn, nn.LayerNorm),\n            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n        )\n\n        self.ff = nn.Sequential(\n            Rearrange('b c ih iw -> b (ih iw) c'),\n            PreNorm(oup, self.ff, nn.LayerNorm),\n            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n        )\n\n    def forward(self, x):\n        if self.downsample:\n            x = self.proj(self.pool1(x)) + self.attn(self.pool2(x))\n        else:\n            x = x + self.attn(x)\n        x = x + self.ff(x)\n        return x\n\n\nclass CoAtNet(nn.Module):\n    def __init__(self, image_size, in_channels, num_blocks, channels, num_classes=1000, block_types=['C', 'C', 'T', 'T']):\n        super().__init__()\n        ih, iw = image_size\n        block = {'C': MBConv, 'T': Transformer}\n\n        self.s0 = self._make_layer(\n            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n        self.s1 = self._make_layer(\n            block[block_types[0]], channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n        self.s2 = self._make_layer(\n            block[block_types[1]], channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n        self.s3 = self._make_layer(\n            block[block_types[2]], channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n        self.s4 = self._make_layer(\n            block[block_types[3]], channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n\n        self.pool = nn.AvgPool2d(ih // 32, 1)\n        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n\n    def forward(self, x):\n        x = self.s0(x)\n        x = self.s1(x)\n        x = self.s2(x)\n        x = self.s3(x)\n        x = self.s4(x)\n\n        x = self.pool(x).view(-1, x.shape[1])\n        x = self.fc(x)\n        return x\n\n    def _make_layer(self, block, inp, oup, depth, image_size):\n        layers = nn.ModuleList([])\n        for i in range(depth):\n            if i == 0:\n                layers.append(block(inp, oup, image_size, downsample=True))\n            else:\n                layers.append(block(oup, oup, image_size))\n        return nn.Sequential(*layers)\n\n\ndef coatnet_0():\n    num_blocks = [2, 2, 3, 5, 2]            # L\n    channels = [64, 96, 192, 384, 768]      # D\n    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n\n\ndef coatnet_1():\n    num_blocks = [2, 2, 6, 14, 2]           # L\n    channels = [64, 96, 192, 384, 768]      # D\n    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n\n\ndef coatnet_2():\n    num_blocks = [2, 2, 6, 14, 2]           # L\n    channels = [128, 128, 256, 512, 1026]   # D\n    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n\n\ndef coatnet_3():\n    num_blocks = [2, 2, 6, 14, 2]           # L\n    channels = [192, 192, 384, 768, 1536]   # D\n    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n\n\ndef coatnet_4():\n    num_blocks = [2, 2, 12, 28, 2]          # L\n    channels = [192, 192, 384, 768, 1536]   # D\n    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"_uuid":"95b71fdb-f186-4410-8823-7d10333da9db","_cell_guid":"965baf2c-9bee-4a64-86ad-277181e13973","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-07-29T09:50:50.013028Z","iopub.execute_input":"2022-07-29T09:50:50.013628Z","iopub.status.idle":"2022-07-29T09:50:50.075771Z","shell.execute_reply.started":"2022-07-29T09:50:50.013591Z","shell.execute_reply":"2022-07-29T09:50:50.074893Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#----------------Same as CoAtNet--------------------------\nimport torch\nimport torch.nn as nn\n\n\nfrom einops import rearrange\nfrom einops.layers.torch import Rearrange\n\nclass PreNorm_IGDN(nn.Module):\n    def __init__(self, dim, fn, norm):\n        super().__init__()\n        self.norm = norm(dim,inverse = True)\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\n\nclass SE(nn.Module):\n    def __init__(self, inp, oup, expansion=0.25):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(oup, int(inp * expansion), bias=False),\n            nn.GELU(),\n            nn.Linear(int(inp * expansion), oup, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\nclass Attention(nn.Module):\n    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n        super().__init__()\n        inner_dim = dim_head * heads\n        project_out = not (heads == 1 and dim_head == inp)\n\n        self.ih, self.iw = image_size\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        # parameter table of relative position bias\n        self.relative_bias_table = nn.Parameter(\n            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n\n        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n        coords = torch.flatten(torch.stack(coords), 1)\n        relative_coords = coords[:, :, None] - coords[:, None, :]\n\n        relative_coords[0] += self.ih - 1\n        relative_coords[1] += self.iw - 1\n        relative_coords[0] *= 2 * self.iw - 1\n        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n        self.register_buffer(\"relative_index\", relative_index)\n\n        self.attend = nn.Softmax(dim=-1)\n        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, oup),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        qkv = self.to_qkv(x).chunk(3, dim=-1)\n        q, k, v = map(lambda t: rearrange(\n            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        # Use \"gather\" for more efficiency on GPUs\n        relative_bias = self.relative_bias_table.gather(\n            0, self.relative_index.repeat(1, self.heads))\n        relative_bias = rearrange(\n            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n        dots = dots + relative_bias\n\n        attn = self.attend(dots)\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        out = self.to_out(out)\n        return out\n\n# -----------------Symmetric CoAtNet Modules--------------\nclass Bicubic_upsampler(nn.Module):\n    def __init__(self,scale_factor,mode):\n      super(Bicubic_upsampler,self).__init__()\n      self.upsampler = nn.functional.interpolate\n      self.scale_factor = scale_factor\n      self.mode = mode\n    def forward(self,x):\n      x = self.upsampler(x,scale_factor = self.scale_factor, mode = self.mode)\n      return x\n\n\nclass InverseTransformer(nn.Module):\n    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, upsample=False, dropout=0.):\n        super().__init__()\n        \n\n        self.ih, self.iw = image_size\n        self.upsample = upsample\n        \n\n        if self.upsample: #We can change the upsampling method at some point.\n          #Maybe using bicubic interpolation might be better\n          \n          self.upsampler = nn.PixelShuffle(2) # 2 is the upsample factor can be a hyperparameter\n          inp = int(inp/4) # after upsampling \n\n\n          # Not needed since pixelshuffle reduces channels while upsampling, Needed when using bicubic interpolation\n          # self.upsampler = Bicubic_upsampler(scale_factor = 2, mode = \"bicubic\")\n          self.proj = nn.ConvTranspose2d(inp, oup, 1, 1, 0, bias=False) \n\n        hidden_dim = int(inp * 4)\n        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n        self.ff = FeedForward(oup, hidden_dim, dropout)\n\n        self.attn = nn.Sequential(\n            Rearrange('b c ih iw -> b (ih iw) c'),\n            PreNorm(inp, self.attn, nn.LayerNorm),\n            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n        )\n\n        self.ff = nn.Sequential(\n            Rearrange('b c ih iw -> b (ih iw) c'),\n            PreNorm(oup, self.ff, nn.LayerNorm),\n            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n        )\n\n    def forward(self, x):\n        if self.upsample:\n            x = self.proj(self.upsampler(x)) + self.attn(self.upsampler(x))\n        else:\n            x = x + self.attn(x)\n        x = x + self.ff(x)\n        return x\n\nclass InverseMBConv(nn.Module):\n    def __init__(self, inp, oup, image_size, upsample=False, expansion=4):\n        super().__init__()\n        self.upsample = upsample\n        stride = 1 if self.upsample == False else 2\n        output_padding = 0 if self.upsample == False else 1\n\n        if self.upsample:\n            self.upsampler = nn.PixelShuffle(2)\n            # self.upsampler = Bicubic_upsampler(scale_factor = 2, mode = \"bicubic\")\n            self.proj = nn.Conv2d(int(inp/4), oup, 1, 1, 0, bias=False)\n\n        hidden_dim = int(inp * expansion)\n        \n          \n        if expansion == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.ConvTranspose2d(inp, hidden_dim, 3, stride,\n                          1, groups=inp, bias=False,output_padding = output_padding),\n                nn.BatchNorm2d(hidden_dim),\n                nn.GELU(),\n                # pw-linear\n                nn.ConvTranspose2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                # down-sample in the first conv\n                nn.ConvTranspose2d(inp, hidden_dim, 1, stride, 0, bias=False,output_padding = output_padding),\n                nn.BatchNorm2d(hidden_dim),\n                nn.GELU(),\n                # dw\n                nn.ConvTranspose2d(hidden_dim, hidden_dim, 3, 1, 1,\n                          groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.GELU(),\n                SE(inp, hidden_dim),\n                # pw-linear\n                nn.ConvTranspose2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        \n        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n\n    def forward(self, x):\n        if self.upsample:\n            return self.proj(self.upsampler(x)) + self.conv(x)\n        else:\n            return x + self.conv(x)\n\ndef Inverse_conv_3x3_bn(inp, oup, image_size, upsample=False):\n    stride = 1 if upsample == False else 2\n    output_padding = 0 if upsample == False else 1\n    return nn.Sequential(\n        nn.ConvTranspose2d(inp, oup, 3, stride, 1, bias=False,output_padding = output_padding),\n        nn.BatchNorm2d(oup),\n        nn.Tanh()\n    )","metadata":{"_uuid":"5237cf34-b143-474f-b140-b22d6a95ec3e","_cell_guid":"628645da-d695-41f9-8ba1-a3777bbe0a81","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-07-29T09:50:50.078410Z","iopub.execute_input":"2022-07-29T09:50:50.079186Z","iopub.status.idle":"2022-07-29T09:50:50.117762Z","shell.execute_reply.started":"2022-07-29T09:50:50.079158Z","shell.execute_reply":"2022-07-29T09:50:50.116741Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\nclass MLP(nn.Module):\n  def __init__(self,inp,oup,dropout = 0.):\n    super().__init__()\n    hidden = int(inp/2)\n    self.fc1 = nn.Linear(inp,hidden)\n    self.act = nn.GELU()\n    self.fc2 = nn.Linear(hidden,oup)\n    self.drop = nn.Dropout(dropout)\n    self.norm_layer1 = nn.BatchNorm2d(inp)\n    self.norm_layer2 = nn.BatchNorm2d(hidden)\n  \n  def forward(self,x):\n#     x = self.norm_layer1(x)\n    x = x.permute(0,2,3,1)\n    x = self.fc1(x)\n    x = self.act(x)\n    x = self.drop(x)\n#     x = x.permute(0,3,1,2)\n#     x = self.norm_layer2(x)\n#     x = x.permute(0,2,3,1)\n    x = self.fc2(x)\n#     x = self.act(x)\n    x = self.drop(x)\n    x = x.permute(0,3,1,2)\n    return x\n\nclass Inverse_MLP(nn.Module):\n  def __init__(self,inp,oup,dropout = 0.):\n    super().__init__()\n    hidden = int(inp*2)\n    self.fc1 = nn.Linear(inp,hidden)\n    self.act = nn.GELU()\n    self.fc2 = nn.Linear(hidden,oup)\n    self.drop = nn.Dropout(dropout)\n    self.norm_layer1 = nn.BatchNorm2d(inp)\n    self.norm_layer2 = nn.BatchNorm2d(hidden)  \n  def forward(self,x):\n#     x = self.norm_layer1(x)\n    x = x.permute(0,2,3,1)\n    x = self.fc1(x)\n    x = self.act(x)\n    x = self.drop(x)\n#     x = x.permute(0,3,1,2)\n#     x = self.norm_layer2(x)\n#     x = x.permute(0,2,3,1)\n    x = self.fc2(x)\n#     x = self.act(x)\n    x = self.drop(x)\n    x = x.permute(0,3,1,2)\n    return x\n\nclass MergedAutoEncoder(nn.Module):\n  def __init__(self):\n    super(MergedAutoEncoder,self).__init__()\n    ######### encoder layers #########\n    #self.a0 =  conv_3x3_bn(3,12,(16,16),downsample = True)\n    self.a0 = self._make_layer_analysis(conv_3x3_bn,3,64,1,(16,16))\n    # self.gdn12 = GDN(12)\n\n    #self.a1 = MBConv(12,48,(8,8),downsample = True)\n    self.a1 = self._make_layer_analysis(MBConv,64,96,2,(8,8))\n    # self.gdn48 = GDN(48)\n\n    #self.a2 = MBConv(48,192,(4,4),downsample = True)\n    self.a2 = self._make_layer_analysis(MBConv,96,384,6,(4,4))\n    # self.gdn192 = GDN(192)\n\n    #self.a3 = Transformer(192,768,(2,2),downsample=True)\n    self.a3 = self._make_layer_analysis(Transformer,384,384,14,(2,2))\n    # self.gdn768 = GDN(768)\n\n    #self.a4 = Transformer(768,3072,(1,1),downsample = True)\n    self.a4 = self._make_layer_analysis(Transformer,384,192,2,(1,1))\n    # self.gdn3072 = GDN(3072)\n\n#     self.compress = MLP(3072,192)\n\n    ######### decoder layers #########\n\n#     self. decompress = Inverse_MLP(192,3072)\n\n    #self.s4 = InverseTransformer(3072,768,(2,2),upsample = True)\n    self.s4 = self._make_layer_synthesis(InverseTransformer,192,384,2,(2,2))\n    # self.igdn768 = GDN(768,inverse=True)\n\n    #self.s3 = InverseTransformer(768,192,(4,4),upsample = True)\n    self.s3 = self._make_layer_synthesis(InverseTransformer,384,384,14,(4,4))\n    # self.igdn192 = GDN(192,inverse=True)\n\n    # self.s2 = InverseMBConv(192,48,(8,8),upsample= True)\n    self.s2 = self._make_layer_synthesis(InverseMBConv,384,48,6,(8,8))\n\n    # self.igdn48= GDN(48,inverse=True)\n\n    # self.s1 = InverseMBConv(48,12,(16,16),upsample = True)\n    self.s1 = self._make_layer_synthesis(InverseMBConv,48,12,2,(16,16))\n    # self.igdn12= GDN(12,inverse=True)\n\n    # self.s0 = Inverse_conv_3x3_bn(12,3,(32,32),upsample = True)\n    self.s0 = self._make_layer_synthesis(Inverse_conv_3x3_bn,12,3,1,(32,32))\n    # self.igdn3= GDN(3,inverse=True)\n\n  def encode(self,x):\n    x = self.a0(x)\n    x = self.a1(x)\n    x = self.a2(x)\n    x = self.a3(x)\n    x = self.a4(x)\n#     x = self.compress(x)\n    return x\n\n  def decode(self,x):\n#     x = self.decompress(x)\n    x = self.s4(x)\n    x = self.s3(x)\n    x = self.s2(x)\n    x = self.s1(x)\n    x = self.s0(x)\n    return x\n\n  def forward(self,x):\n    enc = self.encode(x)\n    x_hat = self.decode(enc)\n    return x_hat\n\n  def _make_layer_analysis(self,block,inp,oup,depth,image_size):\n    layers = nn.ModuleList([])\n    for i in range(depth):\n      if i == 0:\n        layers.append(block(inp,oup,image_size,downsample = True))\n      else:\n        layers.append(block(oup,oup,image_size))\n    return nn.Sequential(*layers)\n  \n  def _make_layer_synthesis(self,block,inp,oup,depth,image_size):\n    layers = nn.ModuleList([])\n    for i in range(depth):\n      if i == 0:\n        layers.append(block(inp,oup,image_size,upsample = True))\n      else:\n        layers.append(block(oup,oup,image_size))\n    return nn.Sequential(*layers)","metadata":{"_uuid":"bc573c40-392e-4cdd-98ba-df0f04e98d47","_cell_guid":"f6500005-df06-41bd-a3b2-877de1d7696b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-07-29T09:50:50.119402Z","iopub.execute_input":"2022-07-29T09:50:50.120251Z","iopub.status.idle":"2022-07-29T09:50:50.146827Z","shell.execute_reply.started":"2022-07-29T09:50:50.120213Z","shell.execute_reply":"2022-07-29T09:50:50.145944Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Main","metadata":{"_uuid":"4b5e5bd5-2034-4d8b-9eeb-23101a0497e6","_cell_guid":"269dd3c9-4f9a-4f25-a8ca-f41440c6eb5f","trusted":true}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.optim.lr_scheduler\nfrom tqdm import tqdm\nimport math\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_transform = transforms.Compose(\n    [transforms.RandomHorizontalFlip(p=0.5),\n     transforms.RandomCrop(32,padding=4),\n     transforms.RandomInvert(p=0.5),\n#      transforms.CenterCrop(1),\n     transforms.ToTensor(),\n     transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n    ])# normalize the image between [0 1]\n\ntest_transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n    ])\n\nbatch_size = 128\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=train_transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=test_transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\n\nmodel = MergedAutoEncoder().to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(),lr = 1e-3)\nscheduler = torch.optim.lr_scheduler.LinearLR(optimizer,total_iters=9)\n\ndef compute_psnr(img1, img2):\n    img1 = img1.astype(np.float64) \n    img2 = img2.astype(np.float64) \n    mse = np.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return \"Same Image\"\n    return 10 * math.log10(1. / mse)\n\n######## Training #########\ndef train(dataloader,model,loss_fn,optimizer):\n  size = len(dataloader.dataset)\n  model.train()\n  for batch, (X,y) in enumerate(dataloader):\n    X,y = X.to(device), y.to(device) # I guess we dont actually need the labels\n    pred = model(X)\n    loss = criterion(pred,X) # The difference between X and  the prediction by model\n    with torch.autograd.set_detect_anomaly(False):\n      optimizer.zero_grad()\n      loss.backward()\n#       torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm = 1.)\n      optimizer.step()\n      if batch % 100 == 0:\n        loss,current = loss.item(), batch * len(X)\n        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\ndef test(dataloader,model,loss_fn):\n  size = len(dataloader.dataset)\n  num_batches = len(dataloader)\n  model.eval()\n  test_loss, correct = 0,0\n  psnr = 0\n  with torch.no_grad():\n    for X,y in dataloader:\n      X,y = X.to(device), y.to(device)\n      pred = model(X)\n      psnr += compute_psnr(pred.cpu().numpy(),X.cpu().numpy())\n      test_loss += criterion(pred,X).item()\n    print(f\"PSNR: {psnr/num_batches}\")\n    print(f\"Test Loss: {test_loss/num_batches}\")\n\nepochs = 100\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(trainloader, model, criterion, optimizer)\n    test(testloader, model, criterion)\n    if (t+1) % 10 == 0:\n      scheduler.step()\n      print(f\"The last LR is {scheduler.get_last_lr()[0]}\")\nprint(\"Done!\")","metadata":{"_uuid":"b49255a4-bc74-42c5-99a5-5135547dfb25","_cell_guid":"03a404e5-82c8-4f86-a7f1-9d77d2d3184b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-07-29T09:50:50.150058Z","iopub.execute_input":"2022-07-29T09:50:50.150348Z","iopub.status.idle":"2022-07-29T11:33:20.127113Z","shell.execute_reply.started":"2022-07-29T09:50:50.150324Z","shell.execute_reply":"2022-07-29T11:33:20.125084Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test = 0 , None\nprint(test)","metadata":{"_uuid":"1ad6f638-f3d1-46d0-88d1-9281cb2b8143","_cell_guid":"be6d7e2b-9ea2-4d51-8af2-76fe3986de22","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-07-29T11:33:20.128648Z","iopub.status.idle":"2022-07-29T11:33:20.129359Z","shell.execute_reply.started":"2022-07-29T11:33:20.129104Z","shell.execute_reply":"2022-07-29T11:33:20.129130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Save model\ntorch.save(model.state_dict(), \"model-data_augmented_tanh-100e-coat2.pth\")\nprint(\"Saved PyTorch Model State to model-data_augmented_tanh-100e-coat2.pth\")","metadata":{"_uuid":"a6790a27-973f-4cee-9fcb-080a0a3781c9","_cell_guid":"9bbb4630-0947-4dc9-ae4a-8e277bbe7877","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-07-29T11:33:20.130685Z","iopub.status.idle":"2022-07-29T11:33:20.131367Z","shell.execute_reply.started":"2022-07-29T11:33:20.131121Z","shell.execute_reply":"2022-07-29T11:33:20.131146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"_uuid":"9c5bc1e0-8603-4a1b-a7f1-60386eb8e79d","_cell_guid":"da7172b6-89f9-4189-a5e9-425282b33522","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-07-29T11:33:24.932327Z","iopub.execute_input":"2022-07-29T11:33:24.932677Z","iopub.status.idle":"2022-07-29T11:33:24.938251Z","shell.execute_reply.started":"2022-07-29T11:33:24.932647Z","shell.execute_reply":"2022-07-29T11:33:24.937150Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"for test_images, test_labels in testloader:  \n    sample_image = test_images[0]    \n    sample_label = test_labels[0]","metadata":{"_uuid":"c6aefa33-cdd5-4538-b550-89429832a35c","_cell_guid":"b0f4cbbb-4fea-493e-8214-7a894b85fefa","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-07-29T11:33:26.369338Z","iopub.execute_input":"2022-07-29T11:33:26.370372Z","iopub.status.idle":"2022-07-29T11:33:28.557253Z","shell.execute_reply.started":"2022-07-29T11:33:26.370324Z","shell.execute_reply":"2022-07-29T11:33:28.556093Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"plt.imshow(sample_image.reshape(3,32,32).permute(1,2,0))","metadata":{"_uuid":"9c586646-6355-4bbc-877f-7f0561eddfba","_cell_guid":"c168c1b6-c895-49e0-b027-b3ede8092c5b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-07-29T11:33:29.053985Z","iopub.execute_input":"2022-07-29T11:33:29.054666Z","iopub.status.idle":"2022-07-29T11:33:29.252274Z","shell.execute_reply.started":"2022-07-29T11:33:29.054620Z","shell.execute_reply":"2022-07-29T11:33:29.251385Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad(): \n  prediction = model(sample_image.unsqueeze(0).to(device))\n  plt.imshow(prediction.cpu().reshape(3,32,32).permute(1,2,0))","metadata":{"_uuid":"4a05163b-c705-4232-847b-8b7d20f7db5e","_cell_guid":"d7486d49-04e3-4220-b76a-b20933a33a17","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-07-29T11:33:33.837958Z","iopub.execute_input":"2022-07-29T11:33:33.838865Z","iopub.status.idle":"2022-07-29T11:33:34.045713Z","shell.execute_reply.started":"2022-07-29T11:33:33.838828Z","shell.execute_reply":"2022-07-29T11:33:34.044812Z"},"trusted":true},"execution_count":10,"outputs":[]}]}